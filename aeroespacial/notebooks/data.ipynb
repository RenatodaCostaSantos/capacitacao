{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44003802",
   "metadata": {},
   "source": [
    "# Pré-processamento de dados\n",
    "\n",
    "## Primeira ideia\n",
    "\n",
    "- Foram feitas 10 simulações para $cl_b$ e $cl_e$ para se associar a um valor de $r$ e $\\tau$.\n",
    "\n",
    "- Para a modelagem, precisamos agrupar esses valores para termos apenas um valor de $cl_b$ e $cl_e$.\n",
    "\n",
    "- Vamos fazer a média das 10 simulações e associar a média, para cada harmonico esférico, aos observáveis $r$ e $\\tau$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a9a970",
   "metadata": {},
   "source": [
    "# Parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4e6de6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'narrow'\n",
    "dataframes = []\n",
    "base_columns = ['cl_e', 'cl_b']\n",
    "base_avg_columns = ['cl_e_avg', 'cl_b_avg']\n",
    "file_range = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dedbd6",
   "metadata": {},
   "source": [
    "# Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055e99f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "def read_and_prepare_files(directory: str, file_range: int, base_columns: List[str]) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Reads a series of files, renames their columns with a unique ID,\n",
    "    and returns a list of the prepared DataFrames.\n",
    "\n",
    "    Args:\n",
    "        directory: The path to the directory containing the files.\n",
    "        file_ids: A list of integers representing the file IDs to process.\n",
    "        base_columns: A list of base column names to be used for renaming.\n",
    "\n",
    "    Returns:\n",
    "        A list of pandas DataFrames, each with uniquely named columns.\n",
    "    \"\"\"\n",
    "    prepared_dfs = []\n",
    "    \n",
    "    for i in range(1, file_range):\n",
    "        file_name = f'cl_cmb_c501_m{i}.dat'\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep='\\s+', header=None)\n",
    "\n",
    "            df.drop(index=[0, 1], inplace=True)\n",
    "            \n",
    "            # Create a unique list of column names for this file\n",
    "            unique_columns = [f'{col}_{i}' for col in base_columns]\n",
    "            df.columns = unique_columns\n",
    "            \n",
    "            prepared_dfs.append(df)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: The file {file_name} was not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while reading {file_name}: {e}\")\n",
    "            \n",
    "    return prepared_dfs\n",
    "\n",
    "\n",
    "def combine_dataframes(dfs: List[pd.DataFrame], axis = int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combines a list of DataFrames side-by-side using pd.concat with axis=1.\n",
    "\n",
    "    Args:\n",
    "        dfs: A list of pandas DataFrames to be joined.\n",
    "\n",
    "    Returns:\n",
    "        A single DataFrame resulting from the horizontal concatenation.\n",
    "    \"\"\"\n",
    "    # Join all DataFrames side-by-side using axis=1\n",
    "    combined_df = pd.concat(dfs, axis= axis)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def calculate_average_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the row-wise average of 'cl_e' and 'cl_b' columns\n",
    "    and returns a new DataFrame with the average columns.\n",
    "\n",
    "    Args:\n",
    "        df: The input DataFrame containing columns with 'cl_e_' and 'cl_b_' prefixes.\n",
    "\n",
    "    Returns:\n",
    "        A new DataFrame with two columns: 'cl_e_avg' and 'cl_b_avg'.\n",
    "    \"\"\"\n",
    "    # Select all columns that start with 'cl_e_'\n",
    "    cl_e_cols = [col for col in df.columns if col.startswith('cl_e_')]\n",
    "\n",
    "    # Select all columns that start with 'cl_b_'\n",
    "    cl_b_cols = [col for col in df.columns if col.startswith('cl_b_')]\n",
    "\n",
    "    # Calculate the average for each group across rows (axis=1)\n",
    "    cl_e_avg = df[cl_e_cols].mean(axis=1)\n",
    "    cl_b_avg = df[cl_b_cols].mean(axis=1)\n",
    "\n",
    "    # Create the new DataFrame with the two average columns\n",
    "    final_df = pd.DataFrame({\n",
    "        'cl_e_avg': cl_e_avg,\n",
    "        'cl_b_avg': cl_b_avg\n",
    "    })\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def create_transposed_dataframe(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transposes a single column from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: The input DataFrame.\n",
    "        column_name: The name of the column to transpose.\n",
    "\n",
    "    Returns:\n",
    "        The transposed DataFrame.\n",
    "    \"\"\"\n",
    "    transposed_df = df[[column_name]].T\n",
    "    return transposed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "098416f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "DataFrame for cl_e_avg shape: (1, 511)\n",
      "DataFrame for cl_e_avg (first 5 columns):\n",
      "                 2         3         4         5         6\n",
      "cl_e_avg  0.030175  0.027245  0.024617  0.019967  0.010551\n",
      "\n",
      "========================================\n",
      "DataFrame for cl_b_avg shape: (1, 511)\n",
      "DataFrame for cl_b_avg (first 5 columns):\n",
      "                 2         3         4         5         6\n",
      "cl_b_avg  0.000179  0.000156  0.000133  0.000102  0.000076\n"
     ]
    }
   ],
   "source": [
    "dfs_list = read_and_prepare_files(directory=directory, file_range= file_range ,base_columns= base_columns)\n",
    "\n",
    "df = combine_dataframes_horizontally(dfs_list)\n",
    "\n",
    "df_avg = calculate_average_columns(df)\n",
    "\n",
    "\n",
    "# Create the DataFrame for cl_e_avg by transposing the column\n",
    "df_e_avg_transposed = create_transposed_dataframe(df_avg, base_avg_columns[0])\n",
    "\n",
    "# Create the DataFrame for cl_e_avg by transposing the column\n",
    "df_b_avg_transposed = create_transposed_dataframe(df_avg, base_avg_columns[1])\n",
    "\n",
    "# Display the shapes to confirm the transformation\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"DataFrame for cl_e_avg shape: {df_e_avg_transposed.shape}\")\n",
    "print(\"DataFrame for cl_e_avg (first 5 columns):\")\n",
    "print(df_e_avg_transposed.iloc[:, :5])\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"DataFrame for cl_b_avg shape: {df_b_avg_transposed.shape}\")\n",
    "print(\"DataFrame for cl_b_avg (first 5 columns):\")\n",
    "print(df_b_avg_transposed.iloc[:, :5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f41c802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_e = []\n",
    "\n",
    "dfs_b = []\n",
    "\n",
    "for i in range(501, 701):\n",
    "\n",
    "    dfs_list = read_and_prepare_files(directory=directory, file_range= file_range ,base_columns= base_columns)\n",
    "\n",
    "    df = combine_dataframes_horizontally(dfs_list)\n",
    "\n",
    "    df_avg = calculate_average_columns(df)\n",
    "\n",
    "    # Create the DataFrame for cl_e_avg by transposing the column\n",
    "    df_e_avg_transposed = create_transposed_dataframe(df_avg, base_avg_columns[0])\n",
    "\n",
    "    dfs_e.append(df_e_avg_transposed)\n",
    "\n",
    "    # Create the DataFrame for cl_e_avg by transposing the column\n",
    "    df_b_avg_transposed = create_transposed_dataframe(df_avg, base_avg_columns[1])\n",
    "    \n",
    "    dfs_b.append(df_b_avg_transposed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc13f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5ef22b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b9b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6b0def17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cl_e_avg</th>\n",
       "      <th>cl_b_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.017487e-02</td>\n",
       "      <td>1.791218e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.724500e-02</td>\n",
       "      <td>1.557736e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.461747e-02</td>\n",
       "      <td>1.329248e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>2.377047e-08</td>\n",
       "      <td>4.515863e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>2.151342e-08</td>\n",
       "      <td>4.041255e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>2.028507e-08</td>\n",
       "      <td>3.969090e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>1.846510e-08</td>\n",
       "      <td>3.614847e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>513 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         cl_e_avg      cl_b_avg\n",
       "0    0.000000e+00  0.000000e+00\n",
       "1    0.000000e+00  0.000000e+00\n",
       "2    3.017487e-02  1.791218e-04\n",
       "3    2.724500e-02  1.557736e-04\n",
       "4    2.461747e-02  1.329248e-04\n",
       "..            ...           ...\n",
       "508  2.377047e-08  4.515863e-10\n",
       "509  2.151342e-08  4.041255e-10\n",
       "510  2.028507e-08  3.969090e-10\n",
       "511  1.846510e-08  3.614847e-10\n",
       "512  0.000000e+00  0.000000e+00\n",
       "\n",
       "[513 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bd8bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6989a148",
   "metadata": {},
   "source": [
    "# VERSÃO KEDRO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd5cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In conf/base/parameters.yml\n",
    "data_directory: \"narrow\"\n",
    "file_ids: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "base_columns: ['cl_e', 'cl_b']\n",
    "\n",
    "# In conf/base/catalog.yml\n",
    "prepared_dfs_list:\n",
    "  type: kedro.extras.datasets.pickle.PickleDataSet\n",
    "  filepath: data/01_raw/prepared_dfs_list.pkl\n",
    "\n",
    "combined_df:\n",
    "  type: pandas.CSVDataSet\n",
    "  filepath: data/02_intermediate/combined_df.csv\n",
    "\n",
    "combined_df:\n",
    "  type: pandas.CSVDataSet\n",
    "  filepath: data/02_intermediate/combined_data.csv # or wherever the combined file is saved\n",
    "\n",
    "final_df:\n",
    "  type: pandas.CSVDataSet\n",
    "  filepath: data/03_primary/averaged_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397b43af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0103d942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b9a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "def read_and_prepare_files(directory: str, file_ids: List[int], base_columns: List[str]) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Reads a series of files, renames their columns with a unique ID,\n",
    "    and returns a list of the prepared DataFrames.\n",
    "\n",
    "    Args:\n",
    "        directory: The path to the directory containing the files.\n",
    "        file_ids: A list of integers representing the file IDs to process.\n",
    "        base_columns: A list of base column names to be used for renaming.\n",
    "\n",
    "    Returns:\n",
    "        A list of pandas DataFrames, each with uniquely named columns.\n",
    "    \"\"\"\n",
    "    prepared_dfs = []\n",
    "    \n",
    "    for i in file_ids:\n",
    "        file_name = f'cl_cmb_c501_m{i}.dat'\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep='\\s+', header=None)\n",
    "            \n",
    "            # Create a unique list of column names for this file\n",
    "            unique_columns = [f'{col}_{i}' for col in base_columns]\n",
    "            df.columns = unique_columns\n",
    "            \n",
    "            prepared_dfs.append(df)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: The file {file_name} was not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while reading {file_name}: {e}\")\n",
    "            \n",
    "    return prepared_dfs\n",
    "\n",
    "def combine_dataframes_horizontally(dfs: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combines a list of DataFrames side-by-side using pd.concat with axis=1.\n",
    "\n",
    "    Args:\n",
    "        dfs: A list of pandas DataFrames to be joined.\n",
    "\n",
    "    Returns:\n",
    "        A single DataFrame resulting from the horizontal concatenation.\n",
    "    \"\"\"\n",
    "    # Join all DataFrames side-by-side using axis=1\n",
    "    combined_df = pd.concat(dfs, axis=1)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def calculate_average_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the row-wise average of 'cl_e' and 'cl_b' columns\n",
    "    and returns a new DataFrame with the average columns.\n",
    "\n",
    "    Args:\n",
    "        df: The input DataFrame containing columns with 'cl_e_' and 'cl_b_' prefixes.\n",
    "\n",
    "    Returns:\n",
    "        A new DataFrame with two columns: 'cl_e_avg' and 'cl_b_avg'.\n",
    "    \"\"\"\n",
    "    # Select all columns that start with 'cl_e_'\n",
    "    cl_e_cols = [col for col in df.columns if col.startswith('cl_e_')]\n",
    "\n",
    "    # Select all columns that start with 'cl_b_'\n",
    "    cl_b_cols = [col for col in df.columns if col.startswith('cl_b_')]\n",
    "\n",
    "    # Calculate the average for each group across rows (axis=1)\n",
    "    cl_e_avg = df[cl_e_cols].mean(axis=1)\n",
    "    cl_b_avg = df[cl_b_cols].mean(axis=1)\n",
    "\n",
    "    # Create the new DataFrame with the two average columns\n",
    "    final_df = pd.DataFrame({\n",
    "        'cl_e_avg': cl_e_avg,\n",
    "        'cl_b_avg': cl_b_avg\n",
    "    })\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ddb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kedro.pipeline import Node, Pipeline\n",
    "\n",
    "from .nodes import read_and_prepare_files, combine_dataframes_horizontally, calculate_average_columns, create_model_input_table, preprocess_companies, preprocess_shuttles\n",
    "\n",
    "\n",
    "def create_pipeline(**kwargs) -> Pipeline:\n",
    "    return Pipeline(\n",
    "        [\n",
    "            Node(\n",
    "                func=read_and_prepare_files,\n",
    "                inputs={\n",
    "                    \"directory\": \"params:data_directory\",\n",
    "                    \"file_ids\": \"params:file_ids\",\n",
    "                    \"base_columns\": \"params:base_columns\"\n",
    "                },\n",
    "                outputs=\"prepared_dfs_list\",\n",
    "                name=\"read_and_prepare_files_node\",\n",
    "            ),\n",
    "            Node(\n",
    "                func=combine_dataframes_horizontally,\n",
    "                inputs=\"prepared_dfs_list\",\n",
    "                outputs=\"combined_df\",\n",
    "                name=\"combine_prepared_dataframes_node\",\n",
    "            ),\n",
    "            Node(\n",
    "                func=calculate_average_columns,\n",
    "                inputs=\"combined_df\",\n",
    "                outputs=\"final_df\",\n",
    "                name=\"calculate_average_columns_node\",\n",
    "            )\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c09e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c05a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe0ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7232f54e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c1b21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c43ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e377ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae34f810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0         1         2             3\n",
      "0       1  0.002872  0.073520  2.182426e-09\n",
      "1       2  0.006368  0.077384  2.199359e-09\n",
      "2       3  0.026655  0.109919  2.347228e-09\n",
      "3       4  0.012663  0.121044  2.400039e-09\n",
      "4       5  0.047298  0.013508  1.935590e-09\n",
      "..    ...       ...       ...           ...\n",
      "995   996  0.036522  0.104435  2.321625e-09\n",
      "996   997  0.010416  0.021746  1.967746e-09\n",
      "997   998  0.046954  0.075001  2.188900e-09\n",
      "998   999  0.011467  0.026458  1.986378e-09\n",
      "999  1000  0.014936  0.051165  2.086998e-09\n",
      "\n",
      "[1000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read a space-separated text file\n",
    "df_wider = pd.read_csv('CosmoID_r_tau_As_1to1000_concat_2dLHsampling_wider.txt', sep='\\s+', header = None)\n",
    "\n",
    "print(df_wider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74be26af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame with headers:\n",
      "       id         r       tau     incognito\n",
      "0       1  0.002872  0.073520  2.182426e-09\n",
      "1       2  0.006368  0.077384  2.199359e-09\n",
      "2       3  0.026655  0.109919  2.347228e-09\n",
      "3       4  0.012663  0.121044  2.400039e-09\n",
      "4       5  0.047298  0.013508  1.935590e-09\n",
      "..    ...       ...       ...           ...\n",
      "995   996  0.036522  0.104435  2.321625e-09\n",
      "996   997  0.010416  0.021746  1.967746e-09\n",
      "997   998  0.046954  0.075001  2.188900e-09\n",
      "998   999  0.011467  0.026458  1.986378e-09\n",
      "999  1000  0.014936  0.051165  2.086998e-09\n",
      "\n",
      "[1000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Assign new headers\n",
    "df_wider.columns = ['id', 'r', 'tau','incognito']\n",
    "\n",
    "print(\"\\nDataFrame with headers:\")\n",
    "\n",
    "print(df_wider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726d3f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 1 elements, new values have 3 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Assign new headers\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdf_wider\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m = [\u001b[33m'\u001b[39m\u001b[33mMomento Multipolo\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtau\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDataFrame with headers:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(df_wider)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projetos/vamo/.venv/lib/python3.11/site-packages/pandas/core/generic.py:6332\u001b[39m, in \u001b[36mNDFrame.__setattr__\u001b[39m\u001b[34m(self, name, value)\u001b[39m\n\u001b[32m   6330\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   6331\u001b[39m     \u001b[38;5;28mobject\u001b[39m.\u001b[34m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n\u001b[32m-> \u001b[39m\u001b[32m6332\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6333\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[32m   6334\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/properties.pyx:69\u001b[39m, in \u001b[36mpandas._libs.properties.AxisProperty.__set__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projetos/vamo/.venv/lib/python3.11/site-packages/pandas/core/generic.py:814\u001b[39m, in \u001b[36mNDFrame._set_axis\u001b[39m\u001b[34m(self, axis, labels)\u001b[39m\n\u001b[32m    809\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    810\u001b[39m \u001b[33;03mThis is called from the cython code when we set the `index` attribute\u001b[39;00m\n\u001b[32m    811\u001b[39m \u001b[33;03mdirectly, e.g. `series.index = [1, 2, 3]`.\u001b[39;00m\n\u001b[32m    812\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    813\u001b[39m labels = ensure_index(labels)\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[38;5;28mself\u001b[39m._clear_item_cache()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projetos/vamo/.venv/lib/python3.11/site-packages/pandas/core/internals/managers.py:238\u001b[39m, in \u001b[36mBaseBlockManager.set_axis\u001b[39m\u001b[34m(self, axis, new_labels)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: AxisInt, new_labels: Index) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    237\u001b[39m     \u001b[38;5;66;03m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_set_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m     \u001b[38;5;28mself\u001b[39m.axes[axis] = new_labels\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projetos/vamo/.venv/lib/python3.11/site-packages/pandas/core/internals/base.py:98\u001b[39m, in \u001b[36mDataManager._validate_set_axis\u001b[39m\u001b[34m(self, axis, new_labels)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m new_len != old_len:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     99\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLength mismatch: Expected axis has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m elements, new \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mvalues have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m elements\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Length mismatch: Expected axis has 1 elements, new values have 3 elements"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9532e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       1  0.002872  0.073520  2.182426e-09\n",
      "0      2  0.006368  0.077384  2.199359e-09\n",
      "1      3  0.026655  0.109919  2.347228e-09\n",
      "2      4  0.012663  0.121044  2.400039e-09\n",
      "3      5  0.047298  0.013508  1.935590e-09\n",
      "4      6  0.036662  0.034989  2.020563e-09\n",
      "..                                     ...\n",
      "694  696  0.006017  0.059273  2.121118e-09\n",
      "695  697  0.027259  0.047538  2.071914e-09\n",
      "696  698  0.016554  0.041973  2.048984e-09\n",
      "697  699  0.012717  0.042370  2.050611e-09\n",
      "698  700  0.025609  0.067076  2.154481e-09\n",
      "\n",
      "[699 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read a space-separated text file\n",
    "df = pd.read_csv('CosmoID_r_tau_As_1to500wider_501to700narrow_2dLHsampling.txt', sep=',')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5bbc4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0             1\n",
      "0    0.000000e+00  0.000000e+00\n",
      "1    0.000000e+00  0.000000e+00\n",
      "2    4.251932e-03  2.276817e-04\n",
      "3    2.081712e-02  7.996054e-05\n",
      "4    7.046895e-03  9.265473e-05\n",
      "..            ...           ...\n",
      "508  2.293751e-08  4.881470e-10\n",
      "509  2.057065e-08  3.901631e-10\n",
      "510  2.099721e-08  4.011278e-10\n",
      "511  1.916048e-08  3.570308e-10\n",
      "512  0.000000e+00  0.000000e+00\n",
      "\n",
      "[513 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read a space-separated text file\n",
    "df = pd.read_csv('narrow/cl_cmb_c501_m1.dat',  sep='\\s+', header = None)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3b0f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame with headers:\n",
      "             Cl_E          Cl_B\n",
      "0    0.000000e+00  0.000000e+00\n",
      "1    0.000000e+00  0.000000e+00\n",
      "2    4.251932e-03  2.276817e-04\n",
      "3    2.081712e-02  7.996054e-05\n",
      "4    7.046895e-03  9.265473e-05\n",
      "..            ...           ...\n",
      "508  2.293751e-08  4.881470e-10\n",
      "509  2.057065e-08  3.901631e-10\n",
      "510  2.099721e-08  4.011278e-10\n",
      "511  1.916048e-08  3.570308e-10\n",
      "512  0.000000e+00  0.000000e+00\n",
      "\n",
      "[513 rows x 2 columns]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eccde54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vamo-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
