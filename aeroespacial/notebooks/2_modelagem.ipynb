{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42948c34",
   "metadata": {},
   "source": [
    "# Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7725989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib #Para salvar o modelo \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.experimental import enable_halving_search_cv # Necessário para importar\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, root_mean_squared_error\n",
    "from scipy.stats import uniform, randint # Para a distribuição de parâmetros\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import pickle\n",
    "import os, random\n",
    "import utils\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5397875c",
   "metadata": {},
   "source": [
    "# Observações:\n",
    "\n",
    "1 - As bibliotecas que serão usadas neste projeto possuem fontes múltiplas de aleatoriedade. Por isso ressalta-se os seguintes pontos abaixo:\n",
    "\n",
    "- numpy, random (do Python), e tensorflow usam geradores diferentes.\n",
    "\n",
    "    - Fixar random_state em HalvingRandomSearchCV só controla o sorteio dos hiperparâmetros, não o comportamento interno da rede.\n",
    "\n",
    "- TensorFlow e paralelismo introduzem aleatoriedade.\n",
    "\n",
    "    - Por padrão, TensorFlow usa múltiplas threads e kernels otimizados (como cuDNN no GPU), que executam operações não determinísticas (principalmente Dropout e Dense).\n",
    "\n",
    "- O KerasRegressor recria o modelo a cada chamada\n",
    "\n",
    "    - Mesmo que o random_state do scikit-learn esteja fixo, cada vez que fit() é chamado, o Sequential() do TensorFlow usa um estado de aleatoriedade independente (a menos que se fixe isso manualmente dentro da função que cria o modelo).\n",
    "\n",
    "Dadas as observações acima, fixa-se abaixo o parâmetro SEED para alguns métodos que serão utilizados neste notebook.\n",
    "\n",
    "\n",
    "\n",
    "2 - Ao rodar o processo de criação do modelo, na minha máquina, com o parâmetro n_jobs diferente de 1, havia o seguinte warning:\n",
    "\n",
    "   \n",
    "    UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. \n",
    "   \n",
    "\n",
    "- Ao parar, todo o processo de seleção de parâmetros ótimos era comprometido e não havia reproducibilidade do modelo à cada rodada.\n",
    "\n",
    "- Colocando o n_jobs = 1, garante-se a reproducibilidade do modelo, ao custo de aumentar bastante o tempo de treinamento do modelo\n",
    "\n",
    "- Caso esse modelo seja rodado em uma máquina mais robusta, esse problema pode não ocorrer, sendo possível acelerar o tempo de treinamento ao mudar-se o valor do parâmetro n_jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40969faf",
   "metadata": {},
   "source": [
    "# Parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f8d92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desativar GPU (garante total determinismo)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# Fixar sementes globais\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Garante execução determinística, desativando otimizações não reprodutíveis\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "# Numero de neuronios de entrada na rede neural\n",
    "neuronios_entrada = 510\n",
    "\n",
    "n_jobs = 1\n",
    "\n",
    "caminho_dados = '../data/02_intermediate/training_b_df.csv'\n",
    "\n",
    "caminho_modelo = '../data/06_models/modelo_notebook_2_all_data.joblib'\n",
    "\n",
    "caminho_features_selecionadas = '../data/08_reporting/selected_features_all_features.npy'\n",
    "\n",
    "caminho_shap = '../data/08_reporting/shap_values_calculados_all_features.npy'\n",
    "\n",
    "caminho_X_train_all_features = '../data/03_primary/X_train_all_features.npy'\n",
    "\n",
    "caminho_y_train_all_features = '../data/03_primary/y_train_all_features.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf3053b",
   "metadata": {},
   "source": [
    "# 1 - Lendo os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79504538",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entrada = pd.read_csv(caminho_dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd3829",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entrada['r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc6255",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_entrada.iloc[:, list(range(0,neuronios_entrada))+ [-1]] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb5f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5c7fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.drop(columns=['r'])\n",
    "y_train = df['r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b269011",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c620fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b4c617",
   "metadata": {},
   "source": [
    "## Observação\n",
    "\n",
    "Neste notebook não haverá seleção de features, logo todas as features serão utilizadas na modelagem\n",
    "\n",
    "Para plotar os gráficos SHAP com a explicabilidade dos modelos, salva-se abaixo as features usadas na modelagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8818c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9827647",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84a7f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(caminho_features_selecionadas, 'wb') as f:\n",
    "    pickle.dump({'list': selected_features}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742b524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checa se o arquivo foi salvo corretamente\n",
    "with open(caminho_features_selecionadas, 'rb') as f:\n",
    "    loaded_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a092c0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data['list']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c221dd5",
   "metadata": {},
   "source": [
    "# 2 - Normalizando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae07d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1. Escalonamento das Features (X) \n",
    "X_scaler = MinMaxScaler()\n",
    "X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "\n",
    "# 2. Escalonamento do Target (Y), se for Regressão Contínua \n",
    "# Reformatar y_train para que o scaler funcione (de Series para 2D array/DataFrame)\n",
    "y_train_2d = y_train.values.reshape(-1, 1)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train_2d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e8213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367aa1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a28453",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cd3f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_scaled.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1800591",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_scaled.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbea728f",
   "metadata": {},
   "source": [
    "## Salvando valores normalizados\n",
    "\n",
    "As features normalizadas serão necessárias no notebook de explicabilidade dos modelos, por isso salva-se abaixo:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b122862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(caminho_X_train_all_features, X_train_scaled)\n",
    "\n",
    "np.save(caminho_y_train_all_features, y_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b9a9c",
   "metadata": {},
   "source": [
    "# 3 - Construindo um modelo de redes neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fcca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma função que recebe como parametro o numero de neuronios_entrada\n",
    "modelo_com_input = partial(utils.criar_modelo_regularizado, neuronios_entrada=neuronios_entrada)\n",
    "\n",
    "# 1. Distribuição de Parâmetros a serem testados (ranges ao invés de listas fixas)\n",
    "param_distributions = {\n",
    "    # Neurônios: número inteiro aleatório entre 32 e 256\n",
    "    'model__neurons': randint(low=32, high=256), \n",
    "    \n",
    "    # Taxa de Aprendizado: valor contínuo aleatório em escala logarítmica\n",
    "    # Ex: entre 1e-4 e 1e-2 (0.0001 e 0.01)\n",
    "    'model__learning_rate': uniform(loc=0.0001, scale=0.0099), \n",
    "     \n",
    "    # Batch Size: valores discretos\n",
    "    'batch_size': [16, 32, 64],\n",
    "\n",
    "    # O dropout_rate ajuda a diminuir o overfitting\n",
    "    'model__dropout_rate': uniform(loc=0.1, scale=0.4), # Testar entre 10% e 50%\n",
    "    \n",
    "    # Epochs: valores discretos (o HRS vai descartar os piores cedo)\n",
    "    'epochs': [5, 10, 20] \n",
    "}\n",
    "\n",
    "# 2. Configurar o KerasRegressor\n",
    "nn_model = KerasRegressor(modelo_com_input, \n",
    "                          verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf86eac",
   "metadata": {},
   "source": [
    "## Atenção\n",
    "\n",
    "- Ao juntar as features com o target, foram colocadas 10 valores de features simulados associadas ao mesmo target. \n",
    "\n",
    "- Ao usar a validação cruzada é preciso ter certeza de que as linhas associadas a um dado valor do target, caiam tanto na validação quanto no treino, evitando assim o vazamento do target. \n",
    "\n",
    "- Para isso utiliza-se o parâmetro `cv=gkf` no HalvingRandomSearch abaixo.\n",
    "\n",
    "- A variável gkf usa a classe GroupKFold para levar em consideração a observação acima e evitar o vazamento do target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aba083",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = y_train.to_numpy().ravel() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b96a5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afb8fb9",
   "metadata": {},
   "source": [
    "Abaixo, verifica-se se realmente não há vazamento de target usando-se o GrupoKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialize o GroupKFold\n",
    "gkf = GroupKFold(n_splits=5) \n",
    "\n",
    "# Itere sobre os splits (apenas a primeira dobra é suficiente para checar)\n",
    "for fold, (train_index, val_index) in enumerate(gkf.split(X_train, y_train, groups=groups)):\n",
    "    \n",
    "    if fold == 0: # Checar apenas o primeiro fold\n",
    "        \n",
    "        # 1. Obter os valores de 'r' (originais, não escalados) para Treino e Validação\n",
    "        r_train_fold = y_train[train_index].to_numpy().ravel()\n",
    "        r_val_fold = y_train[val_index].to_numpy().ravel()\n",
    "        \n",
    "        # 2. Encontrar os valores ÚNICOS de 'r' em cada conjunto\n",
    "        r_unique_train = set(r_train_fold)\n",
    "        r_unique_val = set(r_val_fold)\n",
    "        \n",
    "        # 3. Encontrar a Interseção (os valores vazados)\n",
    "        vazamentos = r_unique_train.intersection(r_unique_val)\n",
    "        \n",
    "        print(f\"--- Checagem do Fold {fold + 1} ---\")\n",
    "        print(f\"Total de valores únicos de 'r' no Treino: {len(r_unique_train)}\")\n",
    "        print(f\"Total de valores únicos de 'r' na Validação: {len(r_unique_val)}\")\n",
    "        print(f\"Valores de 'r' vazando (Interseção): {len(vazamentos)}\")\n",
    "        \n",
    "        if len(vazamentos) == 0:\n",
    "            print(\"✅ GroupKFold está funcionando corretamente: Nenhuma intersecção de valores de 'r'.\")\n",
    "        else:\n",
    "            print(f\"❌ ERRO GRAVE: {len(vazamentos)} valores de 'r' estão vazando! O GroupKFold falhou na divisão dos grupos.\")\n",
    "            print(f\"Valores vazados (Primeiros 5): {list(vazamentos)[:5]}\")\n",
    "            \n",
    "        break # Parar após o primeiro fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e035a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# factor=2: Descarta metade dos candidatos a cada iteração.\n",
    "# candidates: O número total de combinações que serão testadas na 1ª rodada (a maior).\n",
    "hrs = HalvingRandomSearchCV(\n",
    "    estimator=nn_model, \n",
    "    param_distributions=param_distributions, \n",
    "    factor=2, \n",
    "    n_candidates=50,\n",
    "    scoring='r2', \n",
    "    random_state=SEED,\n",
    "    cv=gkf, \n",
    "    verbose=2,\n",
    "    # Quando se trabalha com TensorFlow + GridSearchCV, o paralelismo frequentemente causa crash.\n",
    "    n_jobs= n_jobs # n_jobs = 1 deixa a busca mais lenta, mas estável e reprodutível.\n",
    ")\n",
    "\n",
    "print(\"Iniciando Halving Random Search (Testando as melhores combinações eficientemente)...\")\n",
    "\n",
    "# 4. Executar a busca\n",
    "# O HRS executa a busca e o retreinamento (refit=True)\n",
    "hrs_result = hrs.fit(X_train_scaled, y_train_scaled, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d0f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESULTADOS FINAIS DO HALVING RANDOM SEARCH\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f'Número de features usadas: {len(X_train.columns)}')\n",
    "\n",
    "# Melhor pontuação\n",
    "print(f\"Melhor R² Médio: {hrs_result.best_score_:.4f}\")\n",
    "\n",
    "# Melhor combinação de hiperparâmetros\n",
    "print(\"Melhores Parâmetros:\")\n",
    "print(hrs_result.best_params_)\n",
    "\n",
    "# Obter o melhor modelo treinado\n",
    "best_nn_model = hrs_result.best_estimator_\n",
    "\n",
    "# Salva o  modelo gerado\n",
    "joblib.dump(best_nn_model, caminho_modelo)  \n",
    "\n",
    "# --- AVALIAÇÃO FINAL ---\n",
    "\n",
    "# O reshape é necessário pois o KerasRegressor.predict() retorna 1D por padrão no Scikit-Learn Wrapper\n",
    "y_pred_scaled = best_nn_model.predict(X_train_scaled).reshape(-1, 1)\n",
    "\n",
    "# Faz a transformação inversa para obter os valores em 'r':\n",
    "y_pred_original = y_scaler.inverse_transform(y_pred_scaled)\n",
    "\n",
    "# 4. Prepara o target verdadeiro para comparação. Garante que y_train seja convertido de um pandas Series para um Numpy array, que é o objeto que sai da inverse_transform acima\n",
    "y_true = y_train.values.reshape(-1, 1)\n",
    "\n",
    "# Cálculo das métricas no conjunto de treino\n",
    "final_mse = mean_squared_error(y_true, y_pred_original) \n",
    "final_r2 = r2_score(y_true, y_pred_original)\n",
    "final_mae = mean_absolute_error(y_true, y_pred_original) \n",
    "final_rmse = root_mean_squared_error(y_true, y_pred_original)\n",
    "\n",
    "print(\"\\nMétricas do Melhor Modelo (Avaliadas nos dados completos de Treino):\")\n",
    "print(f\"  MSE (Erro Quadrático Médio): {final_mse:.8f} (Penaliza erros grandes)\")\n",
    "print(f\"  RMSE (Erro Quadrático Médio): {final_rmse:.8f} (Unidade de 'r')\")\n",
    "print(f\"  MAE (Erro Absoluto Médio):   {final_mae:.8f} (Unidade de 'r')\")\n",
    "print(f\"  R2 (Ajuste):                 {final_r2:.4f} (Qualidade do ajuste)\")\n",
    "print(\"=\"*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c643255",
   "metadata": {},
   "source": [
    "## 3.1 Visualizando os resultados\n",
    "\n",
    "Abaixo, são mostradas duas visualizações:\n",
    "\n",
    "- A distribuição dos valores preditos comparados com os reais,\n",
    "\n",
    "- A distribuição dos resíduos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad166a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2987c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf29544",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_original.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dbcb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6527b49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria   a linha de identidade X=Y\n",
    "min_val = min(y_train.min(), y_pred_original.min())\n",
    "max_val = max(y_train.max(), y_pred_original.max())\n",
    "ideal_line = np.linspace(min_val, max_val, 100)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# 1. Scatter Plot dos Resultados\n",
    "plt.scatter(y_train, y_pred_original, alpha=0.6, s=20, label='Previsões')\n",
    "\n",
    "# 2. Linha de Identidade (Ajuste Perfeito)\n",
    "plt.plot(ideal_line, ideal_line, color='red', linestyle='--', linewidth=2, label='Ajuste Perfeito (Y=X)')\n",
    "\n",
    "plt.title(f'Previsões vs. Valores Reais de r (R² Final: {final_r2:.4f})', fontsize=14)\n",
    "plt.xlabel('Valores Reais de r', fontsize=12)\n",
    "plt.ylabel('Valores Previstos de r', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=':', alpha=0.7)\n",
    "plt.gca().set_aspect('equal', adjustable='box') # Garante que os eixos tenham a mesma escala\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d5b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcule os resíduos (True - Predicted)\n",
    "residuals = y_true - y_pred_original\n",
    "\n",
    "# Achata o array para 1D (o formato (N,))\n",
    "residuals_1d = residuals.ravel()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Histograma dos resíduos\n",
    "plt.hist(residuals_1d, bins=30, edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Linha vertical em zero (onde o centro do histograma deveria estar)\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Erro Zero')\n",
    "\n",
    "plt.title('Distribuição dos Resíduos (Erros)', fontsize=14)\n",
    "plt.xlabel('Resíduo (Real r - Previsto r)', fontsize=12)\n",
    "plt.ylabel('Frequência', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=':', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165002c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PREPARAÇÃO ---\n",
    "# Criando um DataFrame para facilitar a agregação\n",
    "# Garantindo que y_train e y_pred_original são 1D para o DataFrame\n",
    "y_true_flat = y_true.ravel()\n",
    "y_pred_flat = y_pred_original.ravel()\n",
    "\n",
    "df_results = pd.DataFrame({\n",
    "    'y_true': y_true_flat,\n",
    "    'y_pred': y_pred_flat\n",
    "})\n",
    "\n",
    "# AGREGAR: Agrupar por 'y_true' e calcular Média e Desvio Padrão para 'y_pred'\n",
    "# 'y_true' (a variável agrupada) será o índice do novo DataFrame\n",
    "df_grouped = df_results.groupby('y_true')['y_pred'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# Renomear as colunas para clareza\n",
    "df_grouped.columns = ['y_true', 'y_pred_mean', 'y_pred_std']\n",
    "\n",
    "\n",
    "# Cálculo do Resíduo Padronizado (em módulos)\n",
    "# Evita a divisão por zero, substituindo desvios padrão zero por um valor pequeno (ex: 1e-6)\n",
    "df_grouped['y_pred_std_safe'] = df_grouped['y_pred_std'].replace(0, 1e-6)\n",
    "df_grouped['Standardized_Residual'] = (\n",
    "    np.abs(df_grouped['y_true'] - df_grouped['y_pred_mean']) / df_grouped['y_pred_std_safe']\n",
    ")\n",
    "\n",
    "# Exibir o resultado da agregação \n",
    "print(\"Dados Agrupados (Exemplo):\")\n",
    "print(df_grouped.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eae06d",
   "metadata": {},
   "source": [
    "## Visualização com barra de erros\n",
    "\n",
    "Os dados utilizados associam 10 valores diferentes das features a um mesmo valor do target. O objetivo é simular os ruídos presentes em observações astronômicas.\n",
    "\n",
    "Uma forma mais útil de visualizar os dados é portanto colapsar os 10 dados referentes à cada valor do target em seu valor médio e desvio padrão.\n",
    "\n",
    "Além disso, plota-se o gráfico de resíduos padronizados, que ajuda  visualizar o número de observações que se distanciam do valor real em mais de um desvio-padrão.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b29943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VISUALIZAÇÃO COM GRÁFICO DE DUPLO EIXO ---\n",
    "\n",
    "# Cria um espaço de figuras com 2 linhas e 1 coluna, compartilhando o eixo X\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    2, 1, \n",
    "    figsize=(8, 10), \n",
    "    sharex=True, # Os dois gráficos compartilham o mesmo eixo X\n",
    "    gridspec_kw={'hspace': 0.05}, # Reduz o espaço entre os subplots\n",
    "    constrained_layout=True \n",
    ")\n",
    "\n",
    "# ==========================================================\n",
    "# GRÁFICO SUPERIOR: Previsão Média vs. Real (Com Barras de Erro)\n",
    "# ==========================================================\n",
    "\n",
    "# 1. Plotar os pontos agregados com Barras de Erro\n",
    "ax1.errorbar(\n",
    "    x=df_grouped['y_true'],             \n",
    "    y=df_grouped['y_pred_mean'],            \n",
    "    yerr=df_grouped['y_pred_std'],          \n",
    "    fmt='o',                                \n",
    "    capsize=4,                              \n",
    "    alpha=0.7,\n",
    "    label='Previsão Média ± 1 DP'\n",
    ")\n",
    "\n",
    "# 2. Linha de Identidade (Ajuste Perfeito Y=X)\n",
    "ax1.plot(ideal_line, ideal_line, color='red', linestyle='--', linewidth=2, label='Ajuste Perfeito (Y=X)')\n",
    "\n",
    "ax1.set_title(f'Análise de Previsão Agregada (R² Final: {final_r2:.4f})', fontsize=14)\n",
    "ax1.set_ylabel('Valores Previstos Médios de r', fontsize=12)\n",
    "ax1.grid(True, linestyle=':', alpha=0.7)\n",
    "ax1.legend()\n",
    "\n",
    "# ==========================================================\n",
    "# GRÁFICO INFERIOR: Resíduos Padronizados\n",
    "# ==========================================================\n",
    "\n",
    "# 1. Scatter Plot dos Resíduos Padronizados\n",
    "ax2.scatter(\n",
    "    df_grouped['y_true'], \n",
    "    df_grouped['Standardized_Residual'], \n",
    "    alpha=0.7, \n",
    "    s=30, \n",
    "    label='|Resíduo| / DP'\n",
    ")\n",
    "\n",
    "# 2. Linha de Referência Crítica (Z-Score = 1.0)\n",
    "ax2.axhline(\n",
    "    y=1.0, \n",
    "    color='red', \n",
    "    linestyle='-', \n",
    "    linewidth=2, \n",
    "    label='Limite de 1 Desvio Padrão'\n",
    ")\n",
    "ax2.axhline(y=2.0, color='orange', linestyle='--', linewidth=1, label='Limite de 2 DP')\n",
    "\n",
    "\n",
    "ax2.set_xlabel('Valores Reais de r (Y_true)', fontsize=12)\n",
    "ax2.set_ylabel('Resíduo Padronizado (|Y - Ŷ| / DP)', fontsize=12)\n",
    "ax2.grid(True, linestyle=':', alpha=0.7)\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a41cd6",
   "metadata": {},
   "source": [
    "# Versão do gráfico em inglês"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144132bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DUAL-AXIS PLOT VISUALIZATION ---\n",
    "\n",
    "# Creates a figure space with 2 rows and 1 column, sharing the X-axis\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    2, 1, \n",
    "    figsize=(8, 10), \n",
    "    sharex=True, # The two plots share the same X-axis\n",
    "    gridspec_kw={'hspace': 0.05}, # Reduces the space between subplots\n",
    "    constrained_layout=True \n",
    ")\n",
    "\n",
    "# ==========================================================\n",
    "# UPPER PLOT: Mean Prediction vs. True (With Error Bars)\n",
    "# ==========================================================\n",
    "\n",
    "# 1. Plot the aggregated points with Error Bars\n",
    "ax1.errorbar(\n",
    "    x=df_grouped['y_true'],             \n",
    "    y=df_grouped['y_pred_mean'],            \n",
    "    yerr=df_grouped['y_pred_std'],          \n",
    "    fmt='o',                                \n",
    "    capsize=4,                              \n",
    "    alpha=0.7,\n",
    "    label='Mean Prediction ± 1 SD' # Previsão Média ± 1 DP\n",
    ")\n",
    "\n",
    "# 2. Identity Line (Perfect Fit Y=X)\n",
    "ax1.plot(ideal_line, ideal_line, color='red', linestyle='--', linewidth=2, label='Perfect Fit (Y=X)') # Ajuste Perfeito (Y=X)\n",
    "\n",
    "ax1.set_title(f'Aggregated Prediction Analysis (Final R²: {final_r2:.4f})', fontsize=14) # Análise de Previsão Agregada\n",
    "ax1.set_ylabel('Mean Predicted r Values', fontsize=12) # Valores Previstos Médios de r\n",
    "ax1.grid(True, linestyle=':', alpha=0.7)\n",
    "ax1.legend()\n",
    "\n",
    "# ==========================================================\n",
    "# LOWER PLOT: Standardized Residuals\n",
    "# ==========================================================\n",
    "\n",
    "# 1. Scatter Plot of Standardized Residuals\n",
    "ax2.scatter(\n",
    "    df_grouped['y_true'], \n",
    "    df_grouped['Standardized_Residual'], \n",
    "    alpha=0.7, \n",
    "    s=30, \n",
    "    label='|Residual| / SD' # |Resíduo| / DP\n",
    ")\n",
    "\n",
    "# 2. Critical Reference Line (Z-Score = 1.0)\n",
    "ax2.axhline(\n",
    "    y=1.0, \n",
    "    color='red', \n",
    "    linestyle='-', \n",
    "    linewidth=2, \n",
    "    label='1 Standard Deviation Limit' # Limite de 1 Desvio Padrão\n",
    ")\n",
    "ax2.axhline(y=2.0, color='orange', linestyle='--', linewidth=1, label='2 SD Limit') # Limite de 2 DP\n",
    "\n",
    "\n",
    "ax2.set_xlabel('True r Values (Y_true)', fontsize=12) # Valores Reais de r (Y_true)\n",
    "ax2.set_ylabel('Standardized Residual (|Y - Ŷ| / SD)', fontsize=12) # Resíduo Padronizado (|Y - Ŷ| / DP)\n",
    "ax2.grid(True, linestyle=':', alpha=0.7)\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c6f51f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capacitacao-DnesvXxz-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
