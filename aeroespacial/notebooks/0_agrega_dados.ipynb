{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ae0e7d3",
   "metadata": {},
   "source": [
    "# Agregando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09d0b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b00167d",
   "metadata": {},
   "source": [
    "## 1 - Familiarizando com os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91ad8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a space-separated text file\n",
    "df = pd.read_csv('../data/01_raw/wider/cl_cmb_c1_m1.dat',  sep=r'\\s+', header = None)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42db138",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c7f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign new headers\n",
    "df.columns = ['cl_e', 'cl_b']\n",
    "\n",
    "print(\"\\nDataFrame with headers:\")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e2090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a space-separated text file\n",
    "df2 = pd.read_csv('../data/01_raw/wider/cl_cmb_c1_m2.dat',  sep=r'\\s+', header = None)\n",
    "\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed46a0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a241d76",
   "metadata": {},
   "source": [
    "### Observações\n",
    "\n",
    "- Não há dados nulos nos dois arquivos analisados.\n",
    "\n",
    "- Observa-se que as duas primeiras linhas são iguais a zero. \n",
    "\n",
    "- Ao conversar com a pessoa que gerou os dados através de uma simulação, ficou recomendado que ambas linhas podem ser excluídas do dataset. \n",
    "\n",
    "- Checa-se mais adiante se o dataframe final, contendo a união de todos os arquivos disponíveis, não terá dados nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff97a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletando colunas desnecessárias\n",
    "df2.drop(index=[0, 1], inplace=True)\n",
    "\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ba7fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign new headers\n",
    "df2.columns = ['cl_e', 'cl_b']\n",
    "\n",
    "print(\"\\nDataFrame with headers:\")\n",
    "\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cce8161",
   "metadata": {},
   "source": [
    "# 2 - Juntando-se os dados\n",
    "\n",
    "Foram disponibilizados vários arquivos de simulação para os valores de multipolos do espectro de potência da radiação cósmica de fundo.\n",
    "\n",
    "Para cada valor de um observável que queremos prever, foram feitas 10 simulações.\n",
    "\n",
    "Abaixo une-se os 10 arquivos que serão referentes a um único valor de observável cosmológico (que será o target).\n",
    "\n",
    "Exclui-se as linhas com valores iguais a 0: linhas 0,1 e 512 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d2ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../data/01_raw/wider/'\n",
    "dataframes = []\n",
    "base_columns = ['cl_e', 'cl_b']\n",
    "\n",
    "for i in range(1, 11):\n",
    "    file_name = f'cl_cmb_c501_m{i}.dat'\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='\\s+', header=None)\n",
    "        # Assign new headers\n",
    "        df.columns = [f'cl_e_r{i}', f'cl_b_r{i}']\n",
    "\n",
    "\n",
    "        df.drop(index=[0, 1, 512], inplace=True)\n",
    "        \n",
    "        dataframes.append(df)\n",
    "        print(f\"Prepared {file_name}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {file_name} was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb34ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all DataFrames side-by-side using axis=1\n",
    "combined_df = pd.concat(dataframes, axis=1)\n",
    "\n",
    "# Print the shape of the final DataFrame to see the total number of columns\n",
    "print(f\"\\nCombined DataFrame has {combined_df.shape[0]} rows and {combined_df.shape[1]} columns.\")\n",
    "\n",
    "# Display the first few rows of the final DataFrame to show the side-by-side join\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d3a7af",
   "metadata": {},
   "source": [
    "# 3 - Transpondo os dados\n",
    "\n",
    "- Para utilizar todos os dados e simular a presença de ruído nos dados, abaixo transpõe-se os dados lidos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7af66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame for cl_e_avg by transposing the column\n",
    "df_transposed = combined_df.T.reset_index()\n",
    "\n",
    "df_transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3951db",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['index'] + [f'l_{i}' for i in range(2,512)]\n",
    "\n",
    "\n",
    "# Assign new headers\n",
    "df_transposed.columns = columns\n",
    "\n",
    "print(\"\\nDataFrame with headers:\")\n",
    "\n",
    "print(df_transposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f93b2",
   "metadata": {},
   "source": [
    "# 4 - Separando o dataframe acima em dois\n",
    "\n",
    "- As 510 colunas do dataframe acima correspondem aos momentos de multipole do modo $E$ e modo $B$ no espectro de potências da CMB.\n",
    "\n",
    "- Os 510 momentos podem ser utilizados para predizer o valor do observável $r$ no caso dos modos $B$ e para predizer o observável $\\tau$ no caso do modo $E$.\n",
    "\n",
    "- Por isso separa-se o dataframe acima em dois e toma-se a transposta dos dataframes resultantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0da7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e = df_transposed.loc[df_transposed['index'].str.startswith('cl_e'),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a99b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3220a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b = df_transposed.loc[df_transposed['index'].str.startswith('cl_b'),:]\n",
    "df_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad036745",
   "metadata": {},
   "source": [
    "# 5 - Incluindo mais dados\n",
    "\n",
    "- O processo acima foi para exemplificar o pré-processamento referente a uma linha das features do dataframe que será usado para a modelagem\n",
    "\n",
    "- Abaixo inclui-se os demais dados seguindo-se a mesma linha de raciocínio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d26245",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../data/01_raw/wider/'\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "# Outer loop for 'j' values\n",
    "for j in range(1, 1001):  # j from 501 to 700\n",
    "\n",
    "\n",
    "    # Inner loop for 'i' values\n",
    "    for i in range(1, 11):  # i from 1 to 10\n",
    "        file_name = f'cl_cmb_c{j}_m{i}.dat'\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, sep='\\s+', header=None)\n",
    "            df.columns = [f'cl_e_realization{i}_target{j}', f'cl_b_realization{i}_target{j}']\n",
    "            df.drop(index=[0, 1,512], inplace=True)\n",
    "            dataframes.append(df)\n",
    "            print(f\"Prepared {file_name}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: The file {file_name} was not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while reading {file_name}: {e}\")\n",
    "\n",
    "if dataframes:\n",
    "    combined_df = pd.concat(dataframes, axis=1)  \n",
    "\n",
    "else:\n",
    "    print(f\"\\nNo dataframes were created for c{j}.\")\n",
    "\n",
    "# Create the DataFrame for cl by transposing the column\n",
    "df_transposed = combined_df.T.reset_index()\n",
    "\n",
    "columns = ['index'] + [f'l_{i}' for i in range(2,512)]\n",
    "\n",
    "# Assign new headers\n",
    "df_transposed.columns = columns   \n",
    "\n",
    "# After the loop, separetes the dataframes into b e e modes\n",
    "df_e = df_transposed.loc[df_transposed['index'].str.startswith('cl_e'),:]\n",
    "df_b = df_transposed.loc[df_transposed['index'].str.startswith('cl_b'),:]\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Final Concatenated DataFrame for cl_e_avg:\")\n",
    "print(df_e)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Final Concatenated DataFrame for cl_b_avg:\")\n",
    "print(df_b)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa41fe7d",
   "metadata": {},
   "source": [
    "# 6 - Juntando os targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d01059",
   "metadata": {},
   "source": [
    "## 6.1 - Extendendo o número de linhas dos targets\n",
    "\n",
    "No dataset da feature, as 10 realizações associadas ao mesmo índice de target estarão associadas as linhas do target abaixo.\n",
    "\n",
    "Portanto, será necessário extender as linhas dos targets antes de juntá-los ao dataframe das features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1930f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/01_raw/targets/CosmoID_r_tau_As_1to1000_concat_2dLHsampling_wider.txt', sep=r'\\s+', header=None)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6161641",
   "metadata": {},
   "source": [
    "Adicionando nome às colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c443a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign new headers\n",
    "df.drop(columns=[0], inplace=True)\n",
    "df.columns = ['r', 'tau', 'As']\n",
    "\n",
    "print(\"\\nDataFrame with headers:\")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9d76ff",
   "metadata": {},
   "source": [
    "Extendendo o número de linhas dos targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d31cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n is the number of times you want to repeat each row\n",
    "n = 10\n",
    "\n",
    "# Use index.repeat() to duplicate the index n times,\n",
    "# then use .loc[] to create the new DataFrame.\n",
    "df_extended = df.loc[df.index.repeat(n)]\n",
    "\n",
    "# Reset the index for a clean 0 to 99 range\n",
    "df_extended = df_extended.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515008e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f69b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before concatenation, reset the index of each DataFrame\n",
    "final_e_reset = df_e.reset_index(drop=True)\n",
    "final_b_reset = df_b.reset_index(drop=True)\n",
    "\n",
    "df_reset = df_extended.reset_index(drop=True)\n",
    "\n",
    "# Now concatenate the DataFrames with the new, unique index\n",
    "final_b = pd.concat([final_b_reset, df_reset], axis=1)\n",
    "final_b.drop(columns=['tau','As'], inplace=True)\n",
    "\n",
    "\n",
    "# Now concatenate the DataFrames with the new, unique index\n",
    "final_e = pd.concat([final_e_reset, df_reset], axis=1)\n",
    "final_e.drop(columns=['r','As'], inplace=True)\n",
    "\n",
    "\n",
    "final_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5df68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba55d25",
   "metadata": {},
   "source": [
    "# Separando entre treino e teste\n",
    "\n",
    "Como o conjunto de dados utilizados acima tem a peculiaridade de ter 10 observações associadas ao mesmo target, abaixo utiza-se a seguinte estratégia:\n",
    "\n",
    "- Divide-se os datasets entre X para as features e y para os targets.\n",
    "\n",
    "- Utiliza-se a classe GroupShuffleSplit para encontrar-se índices agrupados utilizando-se os valores únicos do target para o agrupamento do dataframe.\n",
    "\n",
    "- Filtra-se as features e targets utilizando-se os índices obtidos acima.\n",
    "\n",
    "- Concatena-se as features com os targets para obter um dataframe final.\n",
    "\n",
    "- Aplica-se a divisão entre treino e teste utilizando-se as funções criadas para excutar as etapas descritas acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08168020",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features_e = final_e.drop(columns=['index','tau'])\n",
    "y_target_e = final_e['tau']\n",
    "\n",
    "X_features_b = final_b.drop(columns=['index','r'])\n",
    "y_target_b = final_b['r']\n",
    "\n",
    "# O array 'groups_e' é o valor de 'tau' para cada linha. \n",
    "# O array 'groups_b' é o valor de 'r' para cada linha. \n",
    "# O GroupKFold garantirá que todos os valores idênticos sejam mantidos juntos.\n",
    "groups_e = y_target_e.to_numpy().ravel()\n",
    "groups_b = y_target_b.to_numpy().ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59724fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_indices_gss(X_features: np.ndarray, y_target: np.ndarray, train_ratio: float = 0.8) -> tuple:\n",
    "    \"\"\"\n",
    "    Gera os índices de treino e teste usando GroupShuffleSplit (80/20 por padrão).\n",
    "    \"\"\"\n",
    "    groups = y_target.to_numpy().ravel()\n",
    "    \n",
    "    # GroupShuffleSplit para um único split (n_splits=1) com a proporção train_ratio\n",
    "    gss_splitter = GroupShuffleSplit(\n",
    "        n_splits=1, \n",
    "        train_size=train_ratio, \n",
    "        random_state=42 # Fixa a aleatoriedade da divisão para reprodutibilidade\n",
    "    )\n",
    "    \n",
    "    # GSS.split() retorna um iterador. Pegamos o primeiro (e único) par de índices.\n",
    "    for train_index, test_index in gss_splitter.split(X_features, groups=groups):\n",
    "        return train_index, test_index\n",
    "    \n",
    "    return np.array([]), np.array([])\n",
    "\n",
    "\n",
    "def aplicar_split(X: np.ndarray, y: np.ndarray, \n",
    "                  train_index: np.ndarray, test_index: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Fatia os arrays X e Y nos conjuntos de Treino e Teste usando os índices fornecidos.\n",
    "    \"\"\"\n",
    "    X_train_array = X.iloc[train_index]\n",
    "    X_test_array = X.iloc[test_index]\n",
    "    \n",
    "    y_train_array = y.iloc[train_index]\n",
    "    y_test_array = y.iloc[test_index]\n",
    "    \n",
    "    return X_train_array, X_test_array, y_train_array, y_test_array\n",
    "\n",
    "\n",
    "def montar_dataframe(X_array: np.ndarray, y_array: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Monta um DataFrame combinando features (X) e target (Y).\n",
    "    \"\"\"\n",
    "    #df_features = pd.DataFrame(X_array, columns=colunas_X)\n",
    "    #df_target = pd.DataFrame(y_array, columns=[coluna_y])\n",
    "    \n",
    "    return pd.concat([X_array, y_array], axis=1)\n",
    "\n",
    "def criar_datasets_treino_teste_final(X_features: np.ndarray, \n",
    "                                     y_target: np.ndarray):                        \n",
    "    \"\"\"\n",
    "    Função principal que orquestra o split GKF e a montagem dos DataFrames.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Geração dos Índices \n",
    "    train_index, test_index = gerar_indices_gss(X_features, y_target)\n",
    "    \n",
    "    # 2. Aplicação do Split \n",
    "    X_train, X_test, y_train, y_test = aplicar_split(\n",
    "        X_features, y_target, train_index, test_index\n",
    "    )\n",
    "    \n",
    "    # 3. Montagem dos DataFrames \n",
    "    df_train = montar_dataframe(X_train, y_train)\n",
    "    df_test = montar_dataframe(X_test, y_test)\n",
    "    \n",
    "    return df_train, df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd86bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Processar os Dados do MODO E\n",
    "df_train_E, df_test_E = criar_datasets_treino_teste_final(\n",
    "    X_features=X_features_e, \n",
    "    y_target=y_target_e\n",
    ")\n",
    "\n",
    "# 2. Processar os Dados do MODO B\n",
    "df_train_B, df_test_B = criar_datasets_treino_teste_final(\n",
    "    X_features=X_features_b, \n",
    "    y_target=y_target_b, \n",
    ")\n",
    "\n",
    "print(f\"Modo E - Treino/Teste Shapes: {df_train_E.shape} / {df_test_E.shape}\")\n",
    "print(f\"Modo B - Treino/Teste Shapes: {df_train_B.shape} / {df_test_B.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c8759",
   "metadata": {},
   "source": [
    "## Salvando dados de treino e teste em um arquivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b0d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_E.to_csv('../data/02_intermediate/training_e_df.csv', index=False)\n",
    "df_test_E.to_csv('../data/02_intermediate/teste_e_df.csv', index=False)\n",
    "\n",
    "df_train_B.to_csv('../data/02_intermediate/training_b_df.csv', index=False)\n",
    "df_test_B.to_csv('../data/02_intermediate/teste_b_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aef6fd0",
   "metadata": {},
   "source": [
    "# Checa salvamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69e79ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e = pd.read_csv('../data/02_intermediate/training_e_df.csv')\n",
    "df_e_teste = pd.read_csv('../data/02_intermediate/teste_e_df.csv')\n",
    "df_b = pd.read_csv('../data/02_intermediate/training_b_df.csv')\n",
    "df_b_teste = pd.read_csv('../data/02_intermediate/teste_b_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f72ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a99a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc3bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adeeacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b_teste"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capacitacao-DnesvXxz-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
