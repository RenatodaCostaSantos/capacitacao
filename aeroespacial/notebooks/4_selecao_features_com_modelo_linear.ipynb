{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42948c34",
   "metadata": {},
   "source": [
    "# Engenharia e seleção de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb0c640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os, random\n",
    "import utils\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib #Para salvar o modelo \n",
    "from sklearn.experimental import enable_halving_search_cv # Necessário para importar\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import Input\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import uniform, randint # Para a distribuição de parâmetros\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Importa e configura o Scikit-learn para rotear metadados\n",
    "from sklearn import set_config\n",
    "set_config(enable_metadata_routing=True) # Para garantir o uso do GroupKFold no modelo linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7e8a95",
   "metadata": {},
   "source": [
    "# Observações:\n",
    "\n",
    "1 - As bibliotecas que serão usadas neste projeto possuem fontes múltiplas de aleatoriedade. Por isso ressalta-se os seguintes pontos abaixo:\n",
    "\n",
    "- numpy, random (do Python), e tensorflow usam geradores diferentes.\n",
    "\n",
    "    - Fixar random_state em HalvingRandomSearchCV só controla o sorteio dos hiperparâmetros, não o comportamento interno da rede.\n",
    "\n",
    "- TensorFlow e paralelismo introduzem aleatóriedade.\n",
    "\n",
    "    - Por padrão, TensorFlow usa múltiplas threads e kernels otimizados (como cuDNN no GPU), que executam operações não determinísticas (principalmente Dropout e Dense).\n",
    "\n",
    "- O KerasRegressor recria o modelo a cada chamada\n",
    "\n",
    "    - Mesmo que o random_state do scikit-learn esteja fixo, cada vez que fit() é chamado, o Sequential() do TensorFlow usa um estado de aleatoriedade independente (a menos que se fixe isso manualmente dentro da função que cria o modelo).\n",
    "\n",
    "Dadas as observações acima, fixa-se abaixo o parâmetro SEED para alguns métodos que serão utilizados neste notebook.\n",
    "\n",
    "\n",
    "\n",
    "2 - Ao rodar o processo de criação do modelo, na minha máquina, com o parâmetro n_jobs diferente de 1, havia o seguinte warning:\n",
    "\n",
    "   \n",
    "    UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. \n",
    "   \n",
    "\n",
    "- Ao parar, todo o processo de seleção de parâmetros ótimos era comprometido e não havia reproducibilidade do modelo à cada rodada.\n",
    "\n",
    "- Colocando o n_jobs = 1, garante-se a reproducibilidade do modelo, ao custo de aumentar bastante o tempo de treinamento do modelo\n",
    "\n",
    "- Caso esse modelo seja rodado em uma máquina mais robusta, esse problema pode não ocorrer, sendo possível acelerar o tempo de treinamento ao mudar-se o valor do parâmetro n_jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35b17aa",
   "metadata": {},
   "source": [
    "# Parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dcdc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desativar GPU (garante total determinismo)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# Fixar sementes globais\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Garante execução determinística, desativando otimizações não reprodutíveis\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "\n",
    "n_jobs = 1\n",
    "\n",
    "# Caso queira-se reaproveitar o cálculo (demorado) dos valores do shap e investigar apenas a explicabilidade do modelo\n",
    "existing_model_shap = False\n",
    "\n",
    "caminho_features_selecionadas = '../data/08_reporting/selected_linear_features.npy'\n",
    "\n",
    "caminho_dados = '../data/02_intermediate/training_b_df.csv'\n",
    "\n",
    "caminho_modelo = '../data/06_models/modelo_notebook_4.joblib'\n",
    "\n",
    "caminho_shap = '../data/06_reporting/shap_values_calculados_linear_features.npy'\n",
    "\n",
    "caminho_X_train_linear_features = '../data/03_primary/X_train_linear_features.npy'\n",
    "\n",
    "caminho_y_train_linear_features = '../data/03_primary/y_train_linear_features.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37468dc8",
   "metadata": {},
   "source": [
    "# 1 - Lendo os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79504538",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/02_intermediate/training_b_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234afd5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5c7fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.drop(columns=['r'])\n",
    "y_train = df['r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b269011",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c620fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ed5e64",
   "metadata": {},
   "source": [
    "# 2 - Engenharia de features\n",
    "\n",
    "Ao conversar com a astrofísica Camila Novaes que forneceu os dados simulados estudados aqui, foi sugerida a criação da média ponderada abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4f2d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = utils.cria_feature_media_ponderada(X_train)\n",
    "\n",
    "print(\"Primeiras 5 linhas com a nova feature:\")\n",
    "print(X_train[['cl_2', 'cl_3', 'cl_4', 'cl_511', 'media_ponderada_Cls']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c7d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['media_ponderada_Cls'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c89aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['media_ponderada_Cls'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8cbd4a",
   "metadata": {},
   "source": [
    "# 3 - Seleção de features\n",
    "\n",
    "Antes de seguir com a seleção de feature, verifica-se abaixo o mapa de calor que mede o grau de correlação linear entre as features e o target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f76991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Junta-se as features de treino (X_train) e o target (y_train)\n",
    "df_train_corr = pd.concat([X_train, y_train.rename('r')], axis=1)\n",
    "\n",
    "# 2. Calcula-se a matriz de correlação (entre todas as features e o target)\n",
    "corr_matrix = df_train_corr.corr()\n",
    "\n",
    "# 3. Foca-se apenas na correlação com o Target\n",
    "target_corr = corr_matrix['r'].sort_values(ascending=False)\n",
    "\n",
    "# Opcional: Pega-se apenas as 20 features mais correlacionadas para melhor visualização\n",
    "top_n = 20\n",
    "top_corr_features = target_corr.head(top_n + 1).index.tolist() # +1 para incluir o próprio Target\n",
    "\n",
    "# 4. Cria o mapa de calor\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    corr_matrix.loc[top_corr_features, top_corr_features], # Matriz reduzida para as top N\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap='coolwarm',\n",
    "    cbar=True\n",
    ")\n",
    "plt.title(f'Mapa de Calor das {top_n} Features Mais Correlacionadas com o Target')\n",
    "plt.show()\n",
    "\n",
    "# 5. Imprimir os valores de correlação com o Target (para confirmação)\n",
    "print(\"\\nCorrelação de Pearson com o Target:\")\n",
    "print(target_corr.drop('r').head(10)) # Mostra as 10 mais correlacionadas\n",
    "\n",
    "print(\"\\nCorrelação de Pearson com o Target:\")\n",
    "print(target_corr.drop('r').tail(10)) # Mostra as 10 mais correlacionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1da5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCorrelação de Pearson com o Target:\")\n",
    "print(target_corr.drop('r').tail(10)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88dfbcc",
   "metadata": {},
   "source": [
    "## Observações \n",
    "\n",
    "- Contexto Físico e Escalas Angulares Críticas (l<100): As features que demonstram a correlação linear mais forte estão notavelmente associadas a múltiplos angulares muito baixos (l<100). Esta faixa de l corresponde às maiores escalas angulares observáveis da CMB, onde o sinal é mais uniforme e menos sujeito a perturbações de pequena escala.\n",
    "\n",
    "\n",
    "- Validação da Simulação dos dados: Este resultado também confirma que, apesar dos dados proverem de simulações, as relações encontradas corroboram com a física esperada tanto para as grandes escalas angulares quanto para pequenas escalas angulares da CMB, dando suporte à representatividade dos dados utilizados no modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d889d4",
   "metadata": {},
   "source": [
    "# Seleção de features usando um modelo linear (LASSO)\n",
    "\n",
    "LASSO significa Least Absolute Shrinkage and Selection Operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08a965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = y_train.to_numpy().ravel() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa4db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Escalar os dados\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_lasso = scaler.fit_transform(X_train)\n",
    "X_train_scaled_lasso = pd.DataFrame(X_train_scaled_lasso, columns=X_train.columns)\n",
    "\n",
    "# 2. Configurar o GroupKFold\n",
    "gkf = GroupKFold(n_splits=5) \n",
    "\n",
    "# 3. Encontrar o Alpha ideal (Melhor parâmetro de regularização)\n",
    "lasso_cv = LassoCV(\n",
    "    cv=gkf, \n",
    "    random_state=42, \n",
    "    n_jobs=-1, \n",
    "    max_iter=10000\n",
    ")\n",
    "\n",
    "# 4. Ajustar o modelo com os grupos para evitar vazamento\n",
    "# Agora, o LassoCV saberá como rotear 'groups' para o GroupKFold.\n",
    "lasso_cv.fit(\n",
    "    X_train_scaled_lasso, \n",
    "    y_train.to_numpy().ravel(),\n",
    "    groups=groups \n",
    ")\n",
    "print(f\"\\nMelhor Alpha (Lambda) Encontrado pelo LASSO (com GroupKFold): {lasso_cv.alpha_:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdc2d8a",
   "metadata": {},
   "source": [
    "## Observação:\n",
    "\n",
    "- No contexto do LASSO (e na biblioteca Scikit-learn), Alpha (α) é o nome dado ao parâmetro de regularização (também chamado de Lambda, λ em estatística).\n",
    "\n",
    "- O α controla a \"força da penalidade\" aplicada aos coeficientes do modelo.\n",
    "\n",
    "- Um α alto (α→∞) força mais coeficientes de features a se tornarem exatamente zero. Isso cria um modelo mais simples (com menos features), mas pode causar underfitting (subajuste).\n",
    "\n",
    "- Um α baixo (α→0) torna o modelo muito parecido com a Regressão Linear padrão (OLS), com pouca ou nenhuma penalidade. Isso usa todas as features, mas aumenta o risco de overfitting (sobreajuste)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a98130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapear coeficientes\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': lasso_cv.coef_\n",
    "})\n",
    "\n",
    "# Filtrar apenas as features que NÃO foram zeradas (selecionadas)\n",
    "selected_coefficients = coefficients[coefficients['Coefficient'] != 0].copy()\n",
    "\n",
    "# Calcular o impacto absoluto para ordenação\n",
    "selected_coefficients['Abs_Impact'] = np.abs(selected_coefficients['Coefficient'])\n",
    "\n",
    "# Ordenar por impacto absoluto (do mais importante para o menos importante)\n",
    "selected_coefficients = selected_coefficients.sort_values(\n",
    "    by='Abs_Impact', \n",
    "    ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1. VISUALIZAÇÃO DAS 10 FEATURES MAIS IMPORTANTES\n",
    "# ----------------------------------------------------\n",
    "\n",
    "TOP_N =20\n",
    "top_N_features = selected_coefficients.head(TOP_N)\n",
    "\n",
    "print(f\"\\n--- TOP {TOP_N} Features Mais Importantes (LASSO Coeficientes) ---\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "# Usamos o método .style.format() do Pandas para formatar a saída com duas casas decimais,\n",
    "# garantindo que apenas as colunas de coeficiente sejam formatadas.\n",
    "styled_output = top_N_features.style.format({\n",
    "    'Coefficient': '{:.4f}', # Formato para o valor do coeficiente\n",
    "    'Abs_Impact': '{:.4f}'    # Formato para o impacto absoluto\n",
    "}).hide(axis='index') # Oculta o índice do DataFrame para uma tabela mais limpa\n",
    "\n",
    "print(styled_output.to_string())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x='Abs_Impact', \n",
    "    y='Feature', \n",
    "    data=top_N_features, \n",
    "    hue='Feature', # Variável 'y' mapeada para 'hue'\n",
    "    palette='viridis',\n",
    "    legend=False # Desativa a legenda que seria criada por 'hue'\n",
    ")\n",
    "plt.title(f'Top {TOP_N} Importância de Features (Coeficientes LASSO)', fontsize=14)\n",
    "plt.xlabel('Impacto Absoluto (|Coeficiente|)')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f8cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Obter os coeficientes do modelo\n",
    "coeficientes = lasso_cv.coef_\n",
    "\n",
    "# 2. Obter os nomes das features (certifique-se de que estão na ordem correta)\n",
    "# Exemplo: Se X é um DataFrame\n",
    "nomes_features = X_train.columns\n",
    "\n",
    "# 3. Criar uma máscara booleana para coeficientes não-zero\n",
    "features_selecionadas_mask = coeficientes != 0\n",
    "\n",
    "# 4. Aplicar a máscara para obter os nomes das features\n",
    "selected_feature_names = nomes_features[features_selecionadas_mask]\n",
    "\n",
    "print(\"Total de Features Originais:\", len(nomes_features))\n",
    "print(\"Total de Features Selecionadas pelo LASSO:\", len(selected_feature_names))\n",
    "print(\"\\nFeatures Selecionadas:\\n\", selected_feature_names.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec0a450",
   "metadata": {},
   "source": [
    "### Visualizando a importância das features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe4c782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Obter os Coeficientes\n",
    "coefficients = lasso_cv.coef_\n",
    "\n",
    "# 2. Calcular a Importância Absoluta\n",
    "feature_importance = np.abs(coefficients)\n",
    "\n",
    "# 3. Criar o DataFrame de Importância\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance_Score': feature_importance,\n",
    "    'Coefficient': coefficients  # Incluir o coeficiente original para ver a direção (positiva/negativa)\n",
    "})\n",
    "\n",
    "# 4. Filtrar Features Zeradas e Ordenar\n",
    "selected_features_df = importance_df[importance_df['Importance_Score'] > 1e-6].sort_values(\n",
    "    by='Importance_Score', \n",
    "    ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95fdc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Força a visualização de todas as linhas\n",
    "pd.set_option('display.max_rows', None)\n",
    "# 5. Exibir os Resultados\n",
    "print(\"\\n--- Features Selecionadas e Seus Scores de Importância (LASSO) ---\")\n",
    "selected_features_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reseta a visualização para o máximo de 10 linhas\n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127c1dc7",
   "metadata": {},
   "source": [
    "## Observação\n",
    "\n",
    "Para plotar os gráficos SHAP com a explicabilidade dos modelos, salva-se abaixo as features usadas na modelagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5841521",
   "metadata": {},
   "outputs": [],
   "source": [
    "nomes_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a67da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(caminho_features_selecionadas, 'wb') as f:\n",
    "    pickle.dump({'list': selected_feature_names}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b7a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checa se o arquivo foi salvo corretamente\n",
    "with open(caminho_features_selecionadas, 'rb') as f:\n",
    "    loaded_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2808db30",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data['list']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c06d67",
   "metadata": {},
   "source": [
    "## Observação\n",
    "\n",
    "- Grande parte das features eliminadas pelo modelo linear foram para os valores de 'l' com índice alto, especialmente maiores que 400.\n",
    "\n",
    "- Isso era esperado pela física ser altamente não-linear em pequenas escalas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a54f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[selected_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64318718",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c221dd5",
   "metadata": {},
   "source": [
    "## 4 - Normalizando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae07d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Escalonamento das Features (X) \n",
    "X_scaler = MinMaxScaler()\n",
    "X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "\n",
    "# 2. Escalonamento do Target (Y), se for Regressão Contínua \n",
    "# Reformatar y_train para que o scaler funcione (de Series para 2D array/DataFrame)\n",
    "y_train_2d = y_train.values.reshape(-1, 1)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb184d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367aa1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a28453",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cd3f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_scaled.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1800591",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_scaled.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5199da6",
   "metadata": {},
   "source": [
    "## Salvando valores normalizados\n",
    "\n",
    "As features normalizadas serão necessárias no notebook de explicabilidade dos modelos, por isso salva-se abaixo:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d191ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(caminho_X_train_linear_features, X_train_scaled)\n",
    "\n",
    "np.save(caminho_y_train_linear_features, y_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b9a9c",
   "metadata": {},
   "source": [
    "# 4 - Construindo um modelo de redes neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fcca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero de neuronios de entrada na rede neural\n",
    "neuronios_entrada = len(selected_feature_names)\n",
    "\n",
    "# Cria uma função que já “fixa” neuronios_entrada\n",
    "modelo_com_input = partial(utils.criar_modelo_regularizado, neuronios_entrada=neuronios_entrada)\n",
    "\n",
    "# 1. Distribuição de Parâmetros a serem testados (ranges ao invés de listas fixas)\n",
    "param_distributions = {\n",
    "    # Neurônios: número inteiro aleatório entre 32 e 256\n",
    "    'model__neurons': randint(low=32, high=256), \n",
    "    \n",
    "    # Taxa de Aprendizado: valor contínuo aleatório em escala logarítmica\n",
    "    # Ex: entre 1e-4 e 1e-2 (0.0001 e 0.01)\n",
    "    'model__learning_rate': uniform(loc=0.0001, scale=0.0099), \n",
    "    \n",
    "    # Batch Size: valores discretos\n",
    "    'batch_size': [16, 32, 64],\n",
    "\n",
    "    # O dropout_rate ajuda a diminuir o overfitting\n",
    "    'model__dropout_rate': uniform(loc=0.1, scale=0.4), # Testar entre 10% e 50%\n",
    "    \n",
    "    # Epochs: valores discretos (o HRS vai descartar os piores cedo)\n",
    "    'epochs': [5, 10, 20] \n",
    "}\n",
    "\n",
    "# 2. Configurar o KerasRegressor\n",
    "nn_model = KerasRegressor(model=modelo_com_input, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf86eac",
   "metadata": {},
   "source": [
    "## Atenção\n",
    "\n",
    "- Ao juntar as features com o target, foram colocadas 10 simulações associadas ao mesmo target. \n",
    "\n",
    "- Ao usar a validação cruzada é preciso ter certeza de que as linhas associadas a um dado valor do target, caiam tanto na validação quanto no treino, evitando assim o vazamento do target. \n",
    "\n",
    "- Para isso utiliza-se o parâmetro `cv=gkf` no HalvingRandomSearch abaixo.\n",
    "\n",
    "- A variável gkf usa a classe GroupKFold para levar em consideração a observação acima e evitar o vazamento do target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aba083",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = y_train.to_numpy().ravel() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b96a5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afb8fb9",
   "metadata": {},
   "source": [
    "Abaixo, verifica-se se realmente não há vazamento de target usando-se o GrupoKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialize o GroupKFold\n",
    "gkf = GroupKFold(n_splits=5) \n",
    "\n",
    "# Itere sobre os splits (apenas a primeira dobra é suficiente para checar)\n",
    "for fold, (train_index, val_index) in enumerate(gkf.split(X_train, y_train, groups=groups)):\n",
    "    \n",
    "    if fold == 0: # Checar apenas o primeiro fold\n",
    "        \n",
    "        # 1. Obter os valores de 'r' (originais, não escalados) para Treino e Validação\n",
    "        r_train_fold = y_train[train_index].to_numpy().ravel()\n",
    "        r_val_fold = y_train[val_index].to_numpy().ravel()\n",
    "        \n",
    "        # 2. Encontrar os valores ÚNICOS de 'r' em cada conjunto\n",
    "        r_unique_train = set(r_train_fold)\n",
    "        r_unique_val = set(r_val_fold)\n",
    "        \n",
    "        # 3. Encontrar a Interseção (os valores vazados)\n",
    "        vazamentos = r_unique_train.intersection(r_unique_val)\n",
    "        \n",
    "        print(f\"--- Checagem do Fold {fold + 1} ---\")\n",
    "        print(f\"Total de valores únicos de 'r' no Treino: {len(r_unique_train)}\")\n",
    "        print(f\"Total de valores únicos de 'r' na Validação: {len(r_unique_val)}\")\n",
    "        print(f\"Valores de 'r' vazando (Interseção): {len(vazamentos)}\")\n",
    "        \n",
    "        if len(vazamentos) == 0:\n",
    "            print(\"✅ GroupKFold está funcionando corretamente: Nenhuma intersecção de valores de 'r'.\")\n",
    "        else:\n",
    "            print(f\"❌ ERRO GRAVE: {len(vazamentos)} valores de 'r' estão vazando! O GroupKFold falhou na divisão dos grupos.\")\n",
    "            print(f\"Valores vazados (Primeiros 5): {list(vazamentos)[:5]}\")\n",
    "            \n",
    "        break # Parar após o primeiro fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e035a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# factor=2: Descarta metade dos candidatos a cada iteração.\n",
    "# candidates: O número total de combinações que serão testadas na 1ª rodada (a maior).\n",
    "hrs = HalvingRandomSearchCV(\n",
    "    estimator=nn_model, \n",
    "    param_distributions=param_distributions, \n",
    "    factor=2, \n",
    "    n_candidates=50,\n",
    "    scoring='r2', \n",
    "    random_state=42,\n",
    "    cv=gkf, \n",
    "    verbose=2,\n",
    "    n_jobs=n_jobs\n",
    ")\n",
    "\n",
    "print(\"Iniciando Halving Random Search (Testando as melhores combinações eficientemente)...\")\n",
    "\n",
    "# 4. Executar a busca\n",
    "# O HRS executa a busca e o retreinamento (refit=True)\n",
    "hrs_result = hrs.fit(X_train_scaled, y_train_scaled, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d0f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESULTADOS FINAIS DO HALVING RANDOM SEARCH\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f'Número de features usadas: {len(selected_feature_names)}')\n",
    "\n",
    "# Melhor pontuação\n",
    "print(f\"Melhor R² Médio: {hrs_result.best_score_:.4f}\")\n",
    "\n",
    "# Melhor combinação de hiperparâmetros\n",
    "print(\"Melhores Parâmetros:\")\n",
    "print(hrs_result.best_params_)\n",
    "\n",
    "# Obter o melhor modelo treinado\n",
    "best_nn_model = hrs_result.best_estimator_\n",
    "\n",
    "# Salva o  modelo gerado\n",
    "joblib.dump(best_nn_model, caminho_modelo)\n",
    "\n",
    "# --- AVALIAÇÃO FINAL ---\n",
    "\n",
    "\n",
    "# O reshape é necessário pois o KerasRegressor.predict() retorna 1D por padrão no Scikit-Learn Wrapper\n",
    "y_pred_scaled = best_nn_model.predict(X_train_scaled).reshape(-1, 1)\n",
    "\n",
    "# Faz a transformação inversa para obter os valores em 'r':\n",
    "y_pred_original = y_scaler.inverse_transform(y_pred_scaled)\n",
    "\n",
    "# 4. Prepara o target verdadeiro para comparação. Garante que y_train seja convertido de um pandas Series para um Numpy array, que é o objeto que sai da inverse_transform acima\n",
    "y_true = y_train.values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Cálculo das métricas no conjunto de treino\n",
    "final_mse = mean_squared_error(y_true, y_pred_original) \n",
    "final_rmse = root_mean_squared_error(y_true, y_pred_original) \n",
    "final_r2 = r2_score(y_true, y_pred_original)\n",
    "final_mae = mean_absolute_error(y_true, y_pred_original) \n",
    "\n",
    "\n",
    "print(\"\\nMétricas do Melhor Modelo (Avaliadas nos dados completos de Treino):\")\n",
    "print(f\"  MSE (Erro Quadrático Médio): {final_mse:.8f} (Penaliza erros grandes)\")\n",
    "print(f\"  RMSE (Erro Quadrático Médio): {final_rmse:.8f} (Penaliza erros grandes)\")\n",
    "print(f\"  MAE (Erro Absoluto Médio):   {final_mae:.8f} (Unidade de 'r')\")\n",
    "print(f\"  R2 (Ajuste):                 {final_r2:.4f} (Qualidade do ajuste)\")\n",
    "print(\"=\"*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c67575",
   "metadata": {},
   "source": [
    "## Observação\n",
    "\n",
    "- Comparado com os resultados nos dados de treino do notebook 2, todas as métricas melhoraram.\n",
    "- A seleção de features pelo SelectKBest do notebook 3 ofereceu melhores resultados que o método LASSO usado aqui."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c643255",
   "metadata": {},
   "source": [
    "## 4.1 - Visualizando os resultados\n",
    "\n",
    "Abaixo, são mostradas duas visualizações:\n",
    "\n",
    "- A distribuição dos valores preditos comparados com os reais,\n",
    "\n",
    "- A distribuição dos resíduos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad166a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2987c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf29544",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_original.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dbcb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6527b49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Suponha que você já tem:\n",
    "# y_true_original: valores verdadeiros de 'r' (na escala original)\n",
    "# y_pred_original: valores previstos de 'r' (na escala original)\n",
    "\n",
    "# Crie a linha de identidade X=Y\n",
    "min_val = min(y_train.min(), y_pred_original.min())\n",
    "max_val = max(y_train.max(), y_pred_original.max())\n",
    "ideal_line = np.linspace(min_val, max_val, 100)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# 1. Scatter Plot dos Resultados\n",
    "plt.scatter(y_train, y_pred_original, alpha=0.6, s=20, label='Previsões')\n",
    "\n",
    "# 2. Linha de Identidade (Ajuste Perfeito)\n",
    "plt.plot(ideal_line, ideal_line, color='red', linestyle='--', linewidth=2, label='Ajuste Perfeito (Y=X)')\n",
    "\n",
    "plt.title(f'Previsões vs. Valores Reais de r (R² Final: {final_r2:.4f})', fontsize=14)\n",
    "plt.xlabel('Valores Reais de r', fontsize=12)\n",
    "plt.ylabel('Valores Previstos de r', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=':', alpha=0.7)\n",
    "plt.gca().set_aspect('equal', adjustable='box') # Garante que os eixos tenham a mesma escala\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d5b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcule os resíduos (True - Predicted)\n",
    "residuals = y_true - y_pred_original\n",
    "\n",
    "# Achata o array para 1D (o formato (N,))\n",
    "residuals_1d = residuals.ravel()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Histograma dos resíduos\n",
    "plt.hist(residuals_1d, bins=30, edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Linha vertical em zero (onde o centro do histograma deveria estar)\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Erro Zero')\n",
    "\n",
    "plt.title('Distribuição dos Resíduos (Erros)', fontsize=14)\n",
    "plt.xlabel('Resíduo (Real r - Previsto r)', fontsize=12)\n",
    "plt.ylabel('Frequência', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=':', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8744341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PREPARAÇÃO ---\n",
    "# Criando um DataFrame para facilitar a agregação\n",
    "# Garantindo que y_train e y_pred_original são 1D para o DataFrame\n",
    "y_true_flat = y_true.ravel()\n",
    "y_pred_flat = y_pred_original.ravel()\n",
    "\n",
    "df_results = pd.DataFrame({\n",
    "    'y_true': y_true_flat,\n",
    "    'y_pred': y_pred_flat\n",
    "})\n",
    "\n",
    "# AGREGAR: Agrupar por 'y_true' e calcular Média e Desvio Padrão para 'y_pred'\n",
    "# 'y_true' (a variável agrupada) será o índice do novo DataFrame\n",
    "df_grouped = df_results.groupby('y_true')['y_pred'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# Renomear as colunas para clareza\n",
    "df_grouped.columns = ['y_true', 'y_pred_mean', 'y_pred_std']\n",
    "\n",
    "\n",
    "# Cálculo do Resíduo Padronizado (em módulos, conforme solicitado)\n",
    "# Evite a divisão por zero, substituindo desvios padrão zero por um valor pequeno (ex: 1e-6)\n",
    "df_grouped['y_pred_std_safe'] = df_grouped['y_pred_std'].replace(0, 1e-6)\n",
    "df_grouped['Standardized_Residual'] = (\n",
    "    np.abs(df_grouped['y_true'] - df_grouped['y_pred_mean']) / df_grouped['y_pred_std_safe']\n",
    ")\n",
    "\n",
    "# Exibir o resultado da agregação (opcional)\n",
    "print(\"Dados Agrupados (Exemplo):\")\n",
    "print(df_grouped.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1797cc",
   "metadata": {},
   "source": [
    "## Visualização com barra de erros\n",
    "\n",
    "Os dados utilizados associam 10 valores diferentes das features a um mesmo valor do target. O objetivo é simular os ruídos presentes em observações astronômicas.\n",
    "\n",
    "Uma forma mais útil de visualizar os dados é portanto colapsar os 10 dados referentes à cada valor do target em seu valor médio e desvio padrão.\n",
    "\n",
    "Além disso, plota-se gráfico de resíduos padronizados, que ajuda  visualizar o número de observações que se distanciam do valor real em mais de um desvio-padrão.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305d4aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um espaço de figuras com 2 linhas e 1 coluna, compartilhando o eixo X\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    2, 1, \n",
    "    figsize=(8, 10), \n",
    "    sharex=True, # Os dois gráficos compartilham o mesmo eixo X\n",
    "    gridspec_kw={'hspace': 0.05}, # Reduz o espaço entre os subplots\n",
    "    constrained_layout=True \n",
    ")\n",
    "\n",
    "# ==========================================================\n",
    "# GRÁFICO SUPERIOR: Previsão Média vs. Real (Com Barras de Erro)\n",
    "# ==========================================================\n",
    "\n",
    "# 1. Plotar os pontos agregados com Barras de Erro\n",
    "ax1.errorbar(\n",
    "    x=df_grouped['y_true'],             \n",
    "    y=df_grouped['y_pred_mean'],            \n",
    "    yerr=df_grouped['y_pred_std'],          \n",
    "    fmt='o',                                \n",
    "    capsize=4,                              \n",
    "    alpha=0.7,\n",
    "    label='Previsão Média ± 1 DP'\n",
    ")\n",
    "\n",
    "# 2. Linha de Identidade (Ajuste Perfeito Y=X)\n",
    "ax1.plot(ideal_line, ideal_line, color='red', linestyle='--', linewidth=2, label='Ajuste Perfeito (Y=X)')\n",
    "\n",
    "ax1.set_title(f'Análise de Previsão Agregada (R² Final: {final_r2:.4f})', fontsize=14)\n",
    "ax1.set_ylabel('Valores Previstos Médios de r', fontsize=12)\n",
    "ax1.grid(True, linestyle=':', alpha=0.7)\n",
    "ax1.legend()\n",
    "\n",
    "# ==========================================================\n",
    "# GRÁFICO INFERIOR: Resíduos Padronizados\n",
    "# ==========================================================\n",
    "\n",
    "# 1. Scatter Plot dos Resíduos Padronizados\n",
    "ax2.scatter(\n",
    "    df_grouped['y_true'], \n",
    "    df_grouped['Standardized_Residual'], \n",
    "    alpha=0.7, \n",
    "    s=30, \n",
    "    label='|Resíduo| / DP'\n",
    ")\n",
    "\n",
    "# 2. Linha de Referência Crítica (Z-Score = 1.0)\n",
    "ax2.axhline(\n",
    "    y=1.0, \n",
    "    color='red', \n",
    "    linestyle='-', \n",
    "    linewidth=2, \n",
    "    label='Limite de 1 Desvio Padrão'\n",
    ")\n",
    "ax2.axhline(y=2.0, color='orange', linestyle='--', linewidth=1, label='Limite de 2 DP')\n",
    "\n",
    "\n",
    "ax2.set_xlabel('Valores Reais de r (Y_true)', fontsize=12)\n",
    "ax2.set_ylabel('Resíduo Padronizado (|Y - Ŷ| / DP)', fontsize=12)\n",
    "ax2.grid(True, linestyle=':', alpha=0.7)\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9623d1",
   "metadata": {},
   "source": [
    "# Versão do gráfico em inglês"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362bd72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure space with 2 rows and 1 column, sharing the X-axis\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    2, 1, \n",
    "    figsize=(8, 10), \n",
    "    sharex=True, # The two plots share the same X-axis\n",
    "    gridspec_kw={'hspace': 0.05}, # Reduce space between subplots\n",
    "    constrained_layout=True \n",
    ")\n",
    "\n",
    "# ==========================================================\n",
    "# UPPER PLOT: Mean Prediction vs. True (With Error Bars)\n",
    "# ==========================================================\n",
    "\n",
    "# 1. Plot the aggregated points with Error Bars\n",
    "ax1.errorbar(\n",
    "    x=df_grouped['y_true'],             \n",
    "    y=df_grouped['y_pred_mean'],            \n",
    "    yerr=df_grouped['y_pred_std'],          \n",
    "    fmt='o',                                \n",
    "    capsize=4,                              \n",
    "    alpha=0.7,\n",
    "    label='Mean Prediction ± 1 SD'\n",
    ")\n",
    "\n",
    "# 2. Line of Identity (Perfect Fit Y=X)\n",
    "ax1.plot(ideal_line, ideal_line, color='red', linestyle='--', linewidth=2, label='Perfect Fit (Y=X)')\n",
    "\n",
    "ax1.set_title(f'Aggregate Prediction Analysis (Final R²: {final_r2:.4f})', fontsize=14)\n",
    "ax1.set_ylabel('Mean Predicted r Values', fontsize=12)\n",
    "ax1.grid(True, linestyle=':', alpha=0.7)\n",
    "ax1.legend()\n",
    "\n",
    "# ==========================================================\n",
    "# LOWER PLOT: Standardized Residuals\n",
    "# ==========================================================\n",
    "\n",
    "# 1. Scatter Plot of Standardized Residuals\n",
    "ax2.scatter(\n",
    "    df_grouped['y_true'], \n",
    "    df_grouped['Standardized_Residual'], \n",
    "    alpha=0.7, \n",
    "    s=30, \n",
    "    label='|Residual| / SD'\n",
    ")\n",
    "\n",
    "# 2. Critical Reference Line (Z-Score = 1.0)\n",
    "ax2.axhline(\n",
    "    y=1.0, \n",
    "    color='red', \n",
    "    linestyle='-', \n",
    "    linewidth=2, \n",
    "    label='1 Standard Deviation Limit'\n",
    ")\n",
    "ax2.axhline(y=2.0, color='orange', linestyle='--', linewidth=1, label='2 SD Limit')\n",
    "\n",
    "\n",
    "ax2.set_xlabel('True r Values (Y_true)', fontsize=12)\n",
    "ax2.set_ylabel('Standardized Residual (|Y - Ŷ| / SD)', fontsize=12)\n",
    "ax2.grid(True, linestyle=':', alpha=0.7)\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a891a",
   "metadata": {},
   "source": [
    "## Obsevação\n",
    "\n",
    "- Houve uma pequena melhora nas métricas em relação ao notebook 2, no entanto, o método de seleção estatística do SelectKBest do notebook 3 teve uma performance mais robusta.\n",
    "- É possível que a presença de modos com l>100 estejam confundindo o modelo. Esse fato é esperado pela teoria e confirmado pelas modelagens deste projeto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36a4afd",
   "metadata": {},
   "source": [
    "# 6 - Sugestões para trabalhos futuros\n",
    "\n",
    "- Testar o resultado do modelo eliminando menos/mais features.\n",
    "\n",
    "- Criar novas features com os dados de entrada.\n",
    "\n",
    "- Aumentar a rede de parâmetros da validação cruzada.\n",
    "\n",
    "- Criar uma interface de inferência que receba valores dos cl_s e forneça uma previsão para o valor do target."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capacitacao-DnesvXxz-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
