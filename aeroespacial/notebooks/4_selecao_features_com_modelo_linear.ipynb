{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Engenharia e seleção de features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "import joblib  # Para salvar o modelo\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from scipy.stats import randint, uniform  # Para a distribuição de parâmetros\n",
    "\n",
    "# Importa e configura o Scikit-learn para rotear metadados\n",
    "from sklearn import set_config\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    root_mean_squared_error,\n",
    ")\n",
    "from sklearn.model_selection import GroupKFold, HalvingRandomSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "set_config(\n",
    "    enable_metadata_routing=True\n",
    ")  # Para garantir o uso do GroupKFold no modelo linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Observações:\n",
    "\n",
    "1 - As bibliotecas que serão usadas neste projeto possuem fontes múltiplas de aleatoriedade. Por isso ressalta-se os seguintes pontos abaixo:\n",
    "\n",
    "- numpy, random (do Python), e tensorflow usam geradores diferentes.\n",
    "\n",
    "    - Fixar random_state em HalvingRandomSearchCV só controla o sorteio dos hiperparâmetros, não o comportamento interno da rede.\n",
    "\n",
    "- TensorFlow e paralelismo introduzem aleatóriedade.\n",
    "\n",
    "    - Por padrão, TensorFlow usa múltiplas threads e kernels otimizados (como cuDNN no GPU), que executam operações não determinísticas (principalmente Dropout e Dense).\n",
    "\n",
    "- O KerasRegressor recria o modelo a cada chamada\n",
    "\n",
    "    - Mesmo que o random_state do scikit-learn esteja fixo, cada vez que fit() é chamado, o Sequential() do TensorFlow usa um estado de aleatoriedade independente (a menos que se fixe isso manualmente dentro da função que cria o modelo).\n",
    "\n",
    "Dadas as observações acima, fixa-se abaixo o parâmetro SEED para alguns métodos que serão utilizados neste notebook.\n",
    "\n",
    "\n",
    "\n",
    "2 - Ao rodar o processo de criação do modelo, na minha máquina, com o parâmetro n_jobs diferente de 1, havia o seguinte warning:\n",
    "\n",
    "   \n",
    "    UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. \n",
    "   \n",
    "\n",
    "- Ao parar, todo o processo de seleção de parâmetros ótimos era comprometido e não havia reproducibilidade do modelo à cada rodada.\n",
    "\n",
    "- Colocando o n_jobs = 1, garante-se a reproducibilidade do modelo, ao custo de aumentar bastante o tempo de treinamento do modelo\n",
    "\n",
    "- Caso esse modelo seja rodado em uma máquina mais robusta, esse problema pode não ocorrer, sendo possível acelerar o tempo de treinamento ao mudar-se o valor do parâmetro n_jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desativar GPU (garante total determinismo)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# Fixar sementes globais\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Garante execução determinística, desativando otimizações não reprodutíveis\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "\n",
    "\n",
    "n_jobs = 1\n",
    "\n",
    "# Caso queira-se reaproveitar o cálculo (demorado) dos valores do shap e investigar apenas a explicabilidade do modelo\n",
    "existing_model_shap = False\n",
    "\n",
    "caminho_features_selecionadas = \"../data/08_reporting/selected_linear_features.npy\"\n",
    "\n",
    "caminho_dados = \"../data/02_intermediate/training_b_df.csv\"\n",
    "\n",
    "caminho_modelo = \"../data/06_models/modelo_notebook_4.joblib\"\n",
    "\n",
    "caminho_shap = \"../data/06_reporting/shap_values_calculados_linear_features.npy\"\n",
    "\n",
    "caminho_X_train_linear_features = \"../data/03_primary/X_train_linear_features.npy\"\n",
    "\n",
    "caminho_y_train_linear_features = \"../data/03_primary/y_train_linear_features.npy\"\n",
    "\n",
    "caminho_X_test_linear_features = (\n",
    "    \"../data/03_primary/X_test_final_para_shap_linear_features.npy\"\n",
    ")\n",
    "\n",
    "caminho_y_test_linear_features = (\n",
    "    \"../data/03_primary/y_test_final_para_shap_linear_features.npy\"\n",
    ")\n",
    "\n",
    "caminho_explainer = \"../data/08_reporting/explainer_expected_value_linear_features.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# 1 - Lendo os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/02_intermediate/training_b_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.drop(columns=[\"r\"])\n",
    "y_train = df[\"r\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# 2 - Engenharia de features\n",
    "\n",
    "Ao conversar com a astrofísica Camila Novaes que forneceu os dados simulados estudados aqui, foi sugerida a criação da média ponderada abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = utils.cria_feature_media_ponderada(X_train)\n",
    "\n",
    "print(\"Primeiras 5 linhas com a nova feature:\")\n",
    "print(X_train[[\"l_2\", \"l_3\", \"l_4\", \"l_511\", \"media_ponderada_Cls\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"media_ponderada_Cls\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"media_ponderada_Cls\"].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# 3 - Seleção de features\n",
    "\n",
    "Antes de seguir com a seleção de feature, verifica-se abaixo o mapa de calor que mede o grau de correlação linear entre as features e o target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Junta-se as features de treino (X_train) e o target (y_train)\n",
    "df_train_corr = pd.concat([X_train, y_train.rename(\"r\")], axis=1)\n",
    "\n",
    "# 2. Calcula-se a matriz de correlação (entre todas as features e o target)\n",
    "corr_matrix = df_train_corr.corr()\n",
    "\n",
    "# 3. Foca-se apenas na correlação com o Target\n",
    "target_corr = corr_matrix[\"r\"].sort_values(ascending=False)\n",
    "\n",
    "# Opcional: Pega-se apenas as 20 features mais correlacionadas para melhor visualização\n",
    "top_n = 20\n",
    "top_corr_features = target_corr.head(\n",
    "    top_n + 1\n",
    ").index.tolist()  # +1 para incluir o próprio Target\n",
    "\n",
    "# 4. Cria o mapa de calor\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    corr_matrix.loc[\n",
    "        top_corr_features, top_corr_features\n",
    "    ],  # Matriz reduzida para as top N\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    cbar=True,\n",
    ")\n",
    "plt.title(f\"Mapa de Calor das {top_n} Features Mais Correlacionadas com o Target\")\n",
    "plt.show()\n",
    "\n",
    "# 5. Imprimir os valores de correlação com o Target (para confirmação)\n",
    "print(\"\\nCorrelação de Pearson com o Target:\")\n",
    "print(target_corr.drop(\"r\").head(10))  # Mostra as 10 mais correlacionadas\n",
    "\n",
    "print(\"\\nCorrelação de Pearson com o Target:\")\n",
    "print(target_corr.drop(\"r\").tail(10))  # Mostra as 10 mais correlacionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCorrelação de Pearson com o Target:\")\n",
    "print(target_corr.drop(\"r\").tail(10))  # Mostra as 10 mais correlacionadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Observações \n",
    "\n",
    "- Coerência entre Dados e Modelo: O mapa de calor confirma um alto grau de correlação linear de Pearson entre o target e um subconjunto das features. Esta descoberta é coerente com a análise de explicabilidade anterior, onde os gráficos de dependência SHAP indicaram que o modelo aprendeu predominantemente relações quase lineares com essas features.\n",
    "\n",
    "- Contexto Físico e Escalas Angulares Críticas (l<100): As features que demonstram a correlação linear mais forte estão notavelmente associadas a múltiplos angulares muito baixos (l<100). Esta faixa de l corresponde às maiores escalas angulares observáveis da CMB, onde o sinal é mais uniforme e menos sujeito a perturbações de pequena escala.\n",
    "\n",
    "\n",
    "- Validação da Simulação: Este resultado também confirma que, apesar dos dados proverem de simulações, as relações encontradas corroboram com a física esperada tanto para as grandes escalas angulares quanto para pequenas escalas angulares da CMB, validando a representatividade dos dados utilizados no modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# DUVIDA - PERGUNTAR PARA A CAMILA\n",
    "\n",
    "Para os Cls com valor grande estamos olhando em escalas pequenas, por isso espera-se uma relação não linear entre target e essas features?\n",
    "\n",
    "Pas os Cls com valor pequeno tem-se regiões mais largas de observação no céu, onde a CMB é mais homogenea e isotropica, seria isso a causa da linearidade?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# Seleção de features usando um modelo linear (LASSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = y_train.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Escalar os dados\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_lasso = scaler.fit_transform(X_train)\n",
    "X_train_scaled_lasso = pd.DataFrame(X_train_scaled_lasso, columns=X_train.columns)\n",
    "\n",
    "# 2. Configurar o GroupKFold\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# 3. Encontrar o Alpha ideal (Melhor parâmetro de regularização)\n",
    "lasso_cv = LassoCV(cv=gkf, random_state=42, n_jobs=-1, max_iter=10000)\n",
    "\n",
    "# 4. Ajustar o modelo com os grupos para evitar vazamento\n",
    "# Agora, o LassoCV saberá como rotear 'groups' para o GroupKFold.\n",
    "lasso_cv.fit(X_train_scaled_lasso, y_train.to_numpy().ravel(), groups=groups)\n",
    "print(\n",
    "    f\"\\nMelhor Alpha (Lambda) Encontrado pelo LASSO (com GroupKFold): {lasso_cv.alpha_:.5f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Observação:\n",
    "\n",
    "- No contexto do LASSO (e na biblioteca Scikit-learn), Alpha (α) é o nome dado ao parâmetro de regularização (também chamado de Lambda, λ em estatística).\n",
    "\n",
    "- O α controla a \"força da penalidade\" aplicada aos coeficientes do modelo.\n",
    "\n",
    "- Um α alto (α→∞) força mais coeficientes de features a se tornarem exatamente zero. Isso cria um modelo mais simples (com menos features), mas pode causar underfitting (subajuste).\n",
    "\n",
    "- Um α baixo (α→0) torna o modelo muito parecido com a Regressão Linear padrão (OLS), com pouca ou nenhuma penalidade. Isso usa todas as features, mas aumenta o risco de overfitting (sobreajuste)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapear coeficientes\n",
    "coefficients = pd.DataFrame({\"Feature\": X_train.columns, \"Coefficient\": lasso_cv.coef_})\n",
    "\n",
    "# Filtrar apenas as features que NÃO foram zeradas (selecionadas)\n",
    "selected_coefficients = coefficients[coefficients[\"Coefficient\"] != 0].copy()\n",
    "\n",
    "# Calcular o impacto absoluto para ordenação\n",
    "selected_coefficients[\"Abs_Impact\"] = np.abs(selected_coefficients[\"Coefficient\"])\n",
    "\n",
    "# Ordenar por impacto absoluto (do mais importante para o menos importante)\n",
    "selected_coefficients = selected_coefficients.sort_values(\n",
    "    by=\"Abs_Impact\", ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1. VISUALIZAÇÃO DAS 10 FEATURES MAIS IMPORTANTES\n",
    "# ----------------------------------------------------\n",
    "\n",
    "TOP_N = 20\n",
    "top_N_features = selected_coefficients.head(TOP_N)\n",
    "\n",
    "print(f\"\\n--- TOP {TOP_N} Features Mais Importantes (LASSO Coeficientes) ---\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "# Usamos o método .style.format() do Pandas para formatar a saída com duas casas decimais,\n",
    "# garantindo que apenas as colunas de coeficiente sejam formatadas.\n",
    "styled_output = top_N_features.style.format(\n",
    "    {\n",
    "        \"Coefficient\": \"{:.4f}\",  # Formato para o valor do coeficiente\n",
    "        \"Abs_Impact\": \"{:.4f}\",  # Formato para o impacto absoluto\n",
    "    }\n",
    ").hide(axis=\"index\")  # Oculta o índice do DataFrame para uma tabela mais limpa\n",
    "\n",
    "print(styled_output.to_string())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x=\"Abs_Impact\",\n",
    "    y=\"Feature\",\n",
    "    data=top_N_features,\n",
    "    hue=\"Feature\",  # Variável 'y' mapeada para 'hue'\n",
    "    palette=\"viridis\",\n",
    "    legend=False,  # Desativa a legenda que seria criada por 'hue'\n",
    ")\n",
    "plt.title(f\"Top {TOP_N} Importância de Features (Coeficientes LASSO)\", fontsize=14)\n",
    "plt.xlabel(\"Impacto Absoluto (|Coeficiente|)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Obter os coeficientes do modelo\n",
    "coeficientes = lasso_cv.coef_\n",
    "\n",
    "# 2. Obter os nomes das features (certifique-se de que estão na ordem correta)\n",
    "# Exemplo: Se X é um DataFrame\n",
    "nomes_features = X_train.columns\n",
    "\n",
    "# 3. Criar uma máscara booleana para coeficientes não-zero\n",
    "features_selecionadas_mask = coeficientes != 0\n",
    "\n",
    "# 4. Aplicar a máscara para obter os nomes das features\n",
    "selected_feature_names = nomes_features[features_selecionadas_mask]\n",
    "\n",
    "print(\"Total de Features Originais:\", len(nomes_features))\n",
    "print(\"Total de Features Selecionadas pelo LASSO:\", len(selected_feature_names))\n",
    "print(\"\\nFeatures Selecionadas:\\n\", selected_feature_names.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Observação\n",
    "\n",
    "Para plotar os gráficos SHAP com a explicabilidade dos modelos, salva-se abaixo as features usadas na modelagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "nomes_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(caminho_features_selecionadas, \"wb\") as f:\n",
    "    pickle.dump({\"list\": selected_feature_names}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checa se o arquivo foi salvo corretamente\n",
    "with open(caminho_features_selecionadas, \"rb\") as f:\n",
    "    loaded_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data[\"list\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Observação\n",
    "\n",
    "- Grande parte das features eliminadas pelo modelo linear foram para os valores de 'l' com índice alto, especialmente maiores que 400.\n",
    "\n",
    "- Isso era esperado pela física ser altamente não-linear em pequenas escalas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[selected_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## 4 - Normalizando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Escalonamento das Features (X)\n",
    "X_scaler = MinMaxScaler()\n",
    "X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "\n",
    "# 2. Escalonamento do Target (Y), se for Regressão Contínua\n",
    "# Reformatar y_train para que o scaler funcione (de Series para 2D array/DataFrame)\n",
    "y_train_2d = y_train.values.reshape(-1, 1)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_scaled.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_scaled.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## Salvando valores normalizados\n",
    "\n",
    "As features normalizadas serão necessárias no notebook de explicabilidade dos modelos, por isso salva-se abaixo:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(caminho_X_train_linear_features, X_train_scaled)\n",
    "\n",
    "np.save(caminho_y_train_linear_features, y_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "# 4 - Construindo um modelo de redes neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero de neuronios de entrada na rede neural\n",
    "neuronios_entrada = len(selected_feature_names)\n",
    "\n",
    "# Cria uma função que já “fixa” neuronios_entrada\n",
    "modelo_com_input = partial(\n",
    "    utils.criar_modelo_regularizado, neuronios_entrada=neuronios_entrada\n",
    ")\n",
    "\n",
    "# 1. Distribuição de Parâmetros a serem testados (ranges ao invés de listas fixas)\n",
    "param_distributions = {\n",
    "    # Neurônios: número inteiro aleatório entre 32 e 256\n",
    "    \"model__neurons\": randint(low=32, high=256),\n",
    "    # Taxa de Aprendizado: valor contínuo aleatório em escala logarítmica\n",
    "    # Ex: entre 1e-4 e 1e-2 (0.0001 e 0.01)\n",
    "    \"model__learning_rate\": uniform(loc=0.0001, scale=0.0099),\n",
    "    # Batch Size: valores discretos\n",
    "    \"batch_size\": [16, 32, 64],\n",
    "    # O dropout_rate ajuda a diminuir o overfitting\n",
    "    \"model__dropout_rate\": uniform(loc=0.1, scale=0.4),  # Testar entre 10% e 50%\n",
    "    # Epochs: valores discretos (o HRS vai descartar os piores cedo)\n",
    "    \"epochs\": [5, 10, 20],\n",
    "}\n",
    "\n",
    "# 2. Configurar o KerasRegressor\n",
    "nn_model = KerasRegressor(model=modelo_com_input, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## Atenção\n",
    "\n",
    "- Ao juntar as features com o target, foram colocadas 10 simulações associadas ao mesmo target. \n",
    "\n",
    "- Ao usar a validação cruzada é preciso ter certeza de que as linhas associadas a um dado valor do target, caiam tanto na validação quanto no treino, evitando assim o vazamento do target. \n",
    "\n",
    "- Para isso utiliza-se o parâmetro `cv=gkf` no HalvingRandomSearch abaixo.\n",
    "\n",
    "- A variável gkf usa a classe GroupKFold para levar em consideração a observação acima e evitar o vazamento do target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = y_train.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "Abaixo, verifica-se se realmente não há vazamento de target usando-se o GrupoKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialize o GroupKFold\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Itere sobre os splits (apenas a primeira dobra é suficiente para checar)\n",
    "for fold, (train_index, val_index) in enumerate(\n",
    "    gkf.split(X_train, y_train, groups=groups)\n",
    "):\n",
    "    if fold == 0:  # Checar apenas o primeiro fold\n",
    "        # 1. Obter os valores de 'r' (originais, não escalados) para Treino e Validação\n",
    "        r_train_fold = y_train[train_index].to_numpy().ravel()\n",
    "        r_val_fold = y_train[val_index].to_numpy().ravel()\n",
    "\n",
    "        # 2. Encontrar os valores ÚNICOS de 'r' em cada conjunto\n",
    "        r_unique_train = set(r_train_fold)\n",
    "        r_unique_val = set(r_val_fold)\n",
    "\n",
    "        # 3. Encontrar a Interseção (os valores vazados)\n",
    "        vazamentos = r_unique_train.intersection(r_unique_val)\n",
    "\n",
    "        print(f\"--- Checagem do Fold {fold + 1} ---\")\n",
    "        print(f\"Total de valores únicos de 'r' no Treino: {len(r_unique_train)}\")\n",
    "        print(f\"Total de valores únicos de 'r' na Validação: {len(r_unique_val)}\")\n",
    "        print(f\"Valores de 'r' vazando (Interseção): {len(vazamentos)}\")\n",
    "\n",
    "        if len(vazamentos) == 0:\n",
    "            print(\n",
    "                \"✅ GroupKFold está funcionando corretamente: Nenhuma intersecção de valores de 'r'.\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"❌ ERRO GRAVE: {len(vazamentos)} valores de 'r' estão vazando! O GroupKFold falhou na divisão dos grupos.\"\n",
    "            )\n",
    "            print(f\"Valores vazados (Primeiros 5): {list(vazamentos)[:5]}\")\n",
    "\n",
    "        break  # Parar após o primeiro fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# factor=2: Descarta metade dos candidatos a cada iteração.\n",
    "# candidates: O número total de combinações que serão testadas na 1ª rodada (a maior).\n",
    "hrs = HalvingRandomSearchCV(\n",
    "    estimator=nn_model,\n",
    "    param_distributions=param_distributions,\n",
    "    factor=2,\n",
    "    n_candidates=50,\n",
    "    scoring=\"r2\",\n",
    "    random_state=42,\n",
    "    cv=gkf,\n",
    "    verbose=2,\n",
    "    n_jobs=n_jobs,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Iniciando Halving Random Search (Testando as melhores combinações eficientemente)...\"\n",
    ")\n",
    "\n",
    "# 4. Executar a busca\n",
    "# O HRS executa a busca e o retreinamento (refit=True)\n",
    "hrs_result = hrs.fit(X_train_scaled, y_train_scaled, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"RESULTADOS FINAIS DO HALVING RANDOM SEARCH\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"Número de features usadas: {len(selected_feature_names)}\")\n",
    "\n",
    "# Melhor pontuação\n",
    "print(f\"Melhor R² Médio: {hrs_result.best_score_:.4f}\")\n",
    "\n",
    "# Melhor combinação de hiperparâmetros\n",
    "print(\"Melhores Parâmetros:\")\n",
    "print(hrs_result.best_params_)\n",
    "\n",
    "# Obter o melhor modelo treinado\n",
    "best_nn_model = hrs_result.best_estimator_\n",
    "\n",
    "# Salva o  modelo gerado\n",
    "joblib.dump(best_nn_model, caminho_modelo)\n",
    "\n",
    "# --- AVALIAÇÃO FINAL ---\n",
    "\n",
    "\n",
    "# O reshape é necessário pois o KerasRegressor.predict() retorna 1D por padrão no Scikit-Learn Wrapper\n",
    "y_pred_scaled = best_nn_model.predict(X_train_scaled).reshape(-1, 1)\n",
    "\n",
    "# Faz a transformação inversa para obter os valores em 'r':\n",
    "y_pred_original = y_scaler.inverse_transform(y_pred_scaled)\n",
    "\n",
    "# 4. Prepara o target verdadeiro para comparação. Garante que y_train seja convertido de um pandas Series para um Numpy array, que é o objeto que sai da inverse_transform acima\n",
    "y_true = y_train.values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Cálculo das métricas no conjunto de treino\n",
    "final_mse = mean_squared_error(y_true, y_pred_original)\n",
    "final_rmse = root_mean_squared_error(y_true, y_pred_original)\n",
    "final_r2 = r2_score(y_true, y_pred_original)\n",
    "final_mae = mean_absolute_error(y_true, y_pred_original)\n",
    "\n",
    "\n",
    "print(\"\\nMétricas do Melhor Modelo (Avaliadas nos dados completos de Treino):\")\n",
    "print(f\"  MSE (Erro Quadrático Médio): {final_mse:.8f} (Penaliza erros grandes)\")\n",
    "print(f\"  RMSE (Erro Quadrático Médio): {final_rmse:.8f} (Penaliza erros grandes)\")\n",
    "print(f\"  MAE (Erro Absoluto Médio):   {final_mae:.8f} (Unidade de 'r')\")\n",
    "print(f\"  R2 (Ajuste):                 {final_r2:.4f} (Qualidade do ajuste)\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "## Observação\n",
    "\n",
    "- Comparado com os resultados nos dados de treino do notebook 2, todas as métricas melhoraram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "## 4.1 - Visualizando os resultados\n",
    "\n",
    "Abaixo, são mostradas duas visualizações:\n",
    "\n",
    "- A distribuição dos valores preditos comparados com os reais,\n",
    "\n",
    "- A distribuição dos resíduos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_original.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Suponha que você já tem:\n",
    "# y_true_original: valores verdadeiros de 'r' (na escala original)\n",
    "# y_pred_original: valores previstos de 'r' (na escala original)\n",
    "\n",
    "# Crie a linha de identidade X=Y\n",
    "min_val = min(y_train.min(), y_pred_original.min())\n",
    "max_val = max(y_train.max(), y_pred_original.max())\n",
    "ideal_line = np.linspace(min_val, max_val, 100)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# 1. Scatter Plot dos Resultados\n",
    "plt.scatter(y_train, y_pred_original, alpha=0.6, s=20, label=\"Previsões\")\n",
    "\n",
    "# 2. Linha de Identidade (Ajuste Perfeito)\n",
    "plt.plot(\n",
    "    ideal_line,\n",
    "    ideal_line,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=\"Ajuste Perfeito (Y=X)\",\n",
    ")\n",
    "\n",
    "plt.title(f\"Previsões vs. Valores Reais de r (R² Final: {final_r2:.4f})\", fontsize=14)\n",
    "plt.xlabel(\"Valores Reais de r\", fontsize=12)\n",
    "plt.ylabel(\"Valores Previstos de r\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\":\", alpha=0.7)\n",
    "plt.gca().set_aspect(\n",
    "    \"equal\", adjustable=\"box\"\n",
    ")  # Garante que os eixos tenham a mesma escala\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcule os resíduos (True - Predicted)\n",
    "residuals = y_true - y_pred_original\n",
    "\n",
    "# Achata o array para 1D (o formato (N,))\n",
    "residuals_1d = residuals.ravel()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Histograma dos resíduos\n",
    "plt.hist(residuals_1d, bins=30, edgecolor=\"black\", alpha=0.7)\n",
    "\n",
    "# Linha vertical em zero (onde o centro do histograma deveria estar)\n",
    "plt.axvline(x=0, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Erro Zero\")\n",
    "\n",
    "plt.title(\"Distribuição dos Resíduos (Erros)\", fontsize=14)\n",
    "plt.xlabel(\"Resíduo (Real r - Previsto r)\", fontsize=12)\n",
    "plt.ylabel(\"Frequência\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\":\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PREPARAÇÃO ---\n",
    "# Criando um DataFrame para facilitar a agregação\n",
    "# Garantindo que y_train e y_pred_original são 1D para o DataFrame\n",
    "y_true_flat = y_true.ravel()\n",
    "y_pred_flat = y_pred_original.ravel()\n",
    "\n",
    "df_results = pd.DataFrame({\"y_true\": y_true_flat, \"y_pred\": y_pred_flat})\n",
    "\n",
    "# AGREGAR: Agrupar por 'y_true' e calcular Média e Desvio Padrão para 'y_pred'\n",
    "# 'y_true' (a variável agrupada) será o índice do novo DataFrame\n",
    "df_grouped = df_results.groupby(\"y_true\")[\"y_pred\"].agg([\"mean\", \"std\"]).reset_index()\n",
    "\n",
    "# Renomear as colunas para clareza\n",
    "df_grouped.columns = [\"y_true\", \"y_pred_mean\", \"y_pred_std\"]\n",
    "\n",
    "\n",
    "# Cálculo do Resíduo Padronizado (em módulos, conforme solicitado)\n",
    "# Evite a divisão por zero, substituindo desvios padrão zero por um valor pequeno (ex: 1e-6)\n",
    "df_grouped[\"y_pred_std_safe\"] = df_grouped[\"y_pred_std\"].replace(0, 1e-6)\n",
    "df_grouped[\"Standardized_Residual\"] = (\n",
    "    np.abs(df_grouped[\"y_true\"] - df_grouped[\"y_pred_mean\"])\n",
    "    / df_grouped[\"y_pred_std_safe\"]\n",
    ")\n",
    "\n",
    "# Exibir o resultado da agregação (opcional)\n",
    "print(\"Dados Agrupados (Exemplo):\")\n",
    "print(df_grouped.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "## Visualização com barra de erros\n",
    "\n",
    "Os dados utilizados associam 10 valores diferentes das features a um mesmo valor do target. O objetivo é simular os ruídos presentes em observações astronômicas.\n",
    "\n",
    "Uma forma mais útil de visualizar os dados é portanto colapsar os 10 dados referentes à cada valor do target em seu valor médio e desvio padrão.\n",
    "\n",
    "Além disso, plota-se gráfico de resíduos padronizados, que ajuda  visualizar o número de observações que se distanciam do valor real em mais de um desvio-padrão.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um espaço de figuras com 2 linhas e 1 coluna, compartilhando o eixo X\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    2,\n",
    "    1,\n",
    "    figsize=(8, 10),\n",
    "    sharex=True,  # Os dois gráficos compartilham o mesmo eixo X\n",
    "    gridspec_kw={\"hspace\": 0.05},  # Reduz o espaço entre os subplots\n",
    "    constrained_layout=True,\n",
    ")\n",
    "\n",
    "# ==========================================================\n",
    "# GRÁFICO SUPERIOR: Previsão Média vs. Real (Com Barras de Erro)\n",
    "# ==========================================================\n",
    "\n",
    "# 1. Plotar os pontos agregados com Barras de Erro\n",
    "ax1.errorbar(\n",
    "    x=df_grouped[\"y_true\"],\n",
    "    y=df_grouped[\"y_pred_mean\"],\n",
    "    yerr=df_grouped[\"y_pred_std\"],\n",
    "    fmt=\"o\",\n",
    "    capsize=4,\n",
    "    alpha=0.7,\n",
    "    label=\"Previsão Média ± 1 DP\",\n",
    ")\n",
    "\n",
    "# 2. Linha de Identidade (Ajuste Perfeito Y=X)\n",
    "ax1.plot(\n",
    "    ideal_line,\n",
    "    ideal_line,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=\"Ajuste Perfeito (Y=X)\",\n",
    ")\n",
    "\n",
    "ax1.set_title(f\"Análise de Previsão Agregada (R² Final: {final_r2:.4f})\", fontsize=14)\n",
    "ax1.set_ylabel(\"Valores Previstos Médios de r\", fontsize=12)\n",
    "ax1.grid(True, linestyle=\":\", alpha=0.7)\n",
    "ax1.legend()\n",
    "\n",
    "# ==========================================================\n",
    "# GRÁFICO INFERIOR: Resíduos Padronizados\n",
    "# ==========================================================\n",
    "\n",
    "# 1. Scatter Plot dos Resíduos Padronizados\n",
    "ax2.scatter(\n",
    "    df_grouped[\"y_true\"],\n",
    "    df_grouped[\"Standardized_Residual\"],\n",
    "    alpha=0.7,\n",
    "    s=30,\n",
    "    label=\"|Resíduo| / DP\",\n",
    ")\n",
    "\n",
    "# 2. Linha de Referência Crítica (Z-Score = 1.0)\n",
    "ax2.axhline(\n",
    "    y=1.0, color=\"red\", linestyle=\"-\", linewidth=2, label=\"Limite de 1 Desvio Padrão\"\n",
    ")\n",
    "ax2.axhline(y=2.0, color=\"orange\", linestyle=\"--\", linewidth=1, label=\"Limite de 2 DP\")\n",
    "\n",
    "\n",
    "ax2.set_xlabel(\"Valores Reais de r (Y_true)\", fontsize=12)\n",
    "ax2.set_ylabel(\"Resíduo Padronizado (|Y - Ŷ| / DP)\", fontsize=12)\n",
    "ax2.grid(True, linestyle=\":\", alpha=0.7)\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "## Obsevação\n",
    "\n",
    "- Houve uma melhora nas métricas, mas fica evidente nas visualizações acima que houve uma piora nas previsões para valores muito baixo do target 'r'.\n",
    "\n",
    "- O modelo tende a errar mais para valores de r próximos de 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "# 5 - Aplicando o modelo no conjunto de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = pd.read_csv(\"../data/02_intermediate/teste_b_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = teste.drop(columns=[\"r\"])\n",
    "y_test = teste[\"r\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "## 5.1 - Criando features extras\n",
    "\n",
    "Todas as features criadas para treinar o modelo devem também ser criadas ao se aplicar os dados de teste. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = utils.cria_feature_media_ponderada(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "## 5.2 - Selecionando features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[selected_feature_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "## 5.3 - Normalizando os dados de teste "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = X_scaler.fit_transform(X_test)\n",
    "\n",
    "# Escalonamento do Target (Y)\n",
    "# Reformatar y_train para que o scaler funcione (de Series para 2D array/DataFrame)\n",
    "y_test_2d = y_test.values.reshape(-1, 1)\n",
    "y_test_scaled = y_scaler.fit_transform(y_test_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "## Salvando valores normalizados\n",
    "\n",
    "As features normalizadas serão necessárias no notebook de explicabilidade dos modelos, por isso salva-se abaixo:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(caminho_X_test_linear_features, X_train_scaled)\n",
    "\n",
    "np.save(caminho_y_test_linear_features, y_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtem o melhor modelo do Halving Random Search\n",
    "best_nn_model = hrs_result.best_estimator_\n",
    "\n",
    "# A previsão é feita na escala Z-Score (escalada)\n",
    "y_pred_test_scaled = best_nn_model.predict(X_test_scaled).reshape(-1, 1)\n",
    "\n",
    "# Faz a transformação inversa para obter os valores em 'r':\n",
    "y_pred_test_original = y_scaler.inverse_transform(y_pred_test_scaled)\n",
    "\n",
    "\n",
    "print(\"Previsão no Conjunto de Teste concluída com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_scaled.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_scaled.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_original.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_original.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "## 5. 4 - Visualizando métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo das 3 Métricas de Avaliação Finais:\n",
    "mse_test = mean_squared_error(y_test_2d, y_pred_test_original)\n",
    "rmse_test = root_mean_squared_error(y_test_2d, y_pred_test_original)\n",
    "mae_test = mean_absolute_error(y_test_2d, y_pred_test_original)\n",
    "r2_test = r2_score(y_test_2d, y_pred_test_original)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"AVALIAÇÃO DE DESEMPENHO NO CONJUNTO DE TESTE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"MSE (Erro Quadrático Médio): {mse_test:.8f}\")\n",
    "print(f\"RMSE (Erro Absoluto Médio):   {rmse_test:.8f} (Erro médio na unidade de 'r')\")\n",
    "print(f\"MAE (Erro Absoluto Médio):   {mae_test:.8f} (Erro médio na unidade de 'r')\")\n",
    "print(f\"R2 (Ajuste):                 {r2_test:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie a linha de identidade X=Y\n",
    "min_val = min(y_test_2d.min(), y_pred_test_original.min())\n",
    "max_val = max(y_test_2d.max(), y_pred_test_original.max())\n",
    "ideal_line = np.linspace(min_val, max_val, 100)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# 1. Scatter Plot dos Resultados\n",
    "plt.scatter(y_test_2d, y_pred_test_original, alpha=0.6, s=20, label=\"Previsões\")\n",
    "\n",
    "# 2. Linha de Identidade (Ajuste Perfeito)\n",
    "plt.plot(\n",
    "    ideal_line,\n",
    "    ideal_line,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=\"Ajuste Perfeito (Y=X)\",\n",
    ")\n",
    "\n",
    "plt.title(f\"Previsões vs. Valores Reais de r (R² Final: {r2_test:.4f})\", fontsize=14)\n",
    "plt.xlabel(\"Valores Reais de r\", fontsize=12)\n",
    "plt.ylabel(\"Valores Previstos de r\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\":\", alpha=0.7)\n",
    "plt.gca().set_aspect(\n",
    "    \"equal\", adjustable=\"box\"\n",
    ")  # Garante que os eixos tenham a mesma escala\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcule os resíduos (True - Predicted)\n",
    "residuals = y_test_2d - y_pred_test_original\n",
    "\n",
    "# Achata o array para 1D (o formato (N,))\n",
    "residuals_1d = residuals.ravel()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Histograma dos resíduos\n",
    "plt.hist(residuals_1d, bins=30, edgecolor=\"black\", alpha=0.7)\n",
    "\n",
    "# Linha vertical em zero (onde o centro do histograma deveria estar)\n",
    "plt.axvline(x=0, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Erro Zero\")\n",
    "\n",
    "plt.title(\"Distribuição dos Resíduos (Erros)\", fontsize=14)\n",
    "plt.xlabel(\"Resíduo (Real r - Previsto r)\", fontsize=12)\n",
    "plt.ylabel(\"Frequência\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\":\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "## 5.5 - Visualização com barra de erros\n",
    "\n",
    "Os dados utilizados associam 10 valores diferentes das features a um mesmo valor do target. O objetivo é simular os ruídos presentes em observações astronômicas.\n",
    "\n",
    "A visualização com barras de erros também é feita abaixo utilizando-se os dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PREPARAÇÃO ---\n",
    "# Criando um DataFrame para facilitar a agregação\n",
    "# Garantindo que y_train e y_pred_original são 1D para o DataFrame\n",
    "y_test_flat = y_test_2d.ravel()\n",
    "y_pred_test_flat = y_pred_test_original.ravel()\n",
    "\n",
    "df_results = pd.DataFrame({\"y_test\": y_test_flat, \"y_test_pred\": y_pred_test_flat})\n",
    "\n",
    "# AGREGAR: Agrupar por 'y_true' e calcular Média e Desvio Padrão para 'y_pred'\n",
    "# 'y_true' (a variável agrupada) será o índice do novo DataFrame\n",
    "df_grouped = (\n",
    "    df_results.groupby(\"y_test\")[\"y_test_pred\"].agg([\"mean\", \"std\"]).reset_index()\n",
    ")\n",
    "\n",
    "# Renomear as colunas para clareza\n",
    "df_grouped.columns = [\"y_test\", \"y_test_pred_mean\", \"y_test_pred_std\"]\n",
    "\n",
    "# Cálculo do Resíduo Padronizado (em módulos, conforme solicitado)\n",
    "# Evite a divisão por zero, substituindo desvios padrão zero por um valor pequeno (ex: 1e-6)\n",
    "df_grouped[\"y_test_pred_std_safe\"] = df_grouped[\"y_test_pred_std\"].replace(0, 1e-6)\n",
    "df_grouped[\"Standardized_Residual\"] = (\n",
    "    np.abs(df_grouped[\"y_test\"] - df_grouped[\"y_test_pred_mean\"])\n",
    "    / df_grouped[\"y_test_pred_std_safe\"]\n",
    ")\n",
    "\n",
    "# Exibir o resultado da agregação (opcional)\n",
    "print(\"Dados Agrupados (Exemplo):\")\n",
    "print(df_grouped.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VISUALIZAÇÃO COM GRÁFICO DE DUPLO EIXO ---\n",
    "\n",
    "# Cria um espaço de figuras com 2 linhas e 1 coluna, compartilhando o eixo X\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    2,\n",
    "    1,\n",
    "    figsize=(8, 10),\n",
    "    sharex=True,  # Os dois gráficos compartilham o mesmo eixo X\n",
    "    gridspec_kw={\"hspace\": 0.05},  # Reduz o espaço entre os subplots\n",
    "    constrained_layout=True,\n",
    ")\n",
    "\n",
    "# ==========================================================\n",
    "# GRÁFICO SUPERIOR: Previsão Média vs. Real (Com Barras de Erro)\n",
    "# ==========================================================\n",
    "\n",
    "# 1. Plotar os pontos agregados com Barras de Erro\n",
    "ax1.errorbar(\n",
    "    x=df_grouped[\"y_test\"],\n",
    "    y=df_grouped[\"y_test_pred_mean\"],\n",
    "    yerr=df_grouped[\"y_test_pred_std\"],\n",
    "    fmt=\"o\",\n",
    "    capsize=4,\n",
    "    alpha=0.7,\n",
    "    label=\"Previsão Média ± 1 DP\",\n",
    ")\n",
    "\n",
    "# 2. Linha de Identidade (Ajuste Perfeito Y=X)\n",
    "ax1.plot(\n",
    "    ideal_line,\n",
    "    ideal_line,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=\"Ajuste Perfeito (Y=X)\",\n",
    ")\n",
    "\n",
    "ax1.set_title(f\"Análise de Previsão Agregada (R² Final: {final_r2:.4f})\", fontsize=14)\n",
    "ax1.set_ylabel(\"Valores Previstos Médios de r\", fontsize=12)\n",
    "ax1.grid(True, linestyle=\":\", alpha=0.7)\n",
    "ax1.legend()\n",
    "\n",
    "# ==========================================================\n",
    "# GRÁFICO INFERIOR: Resíduos Padronizados\n",
    "# ==========================================================\n",
    "\n",
    "# 1. Scatter Plot dos Resíduos Padronizados\n",
    "ax2.scatter(\n",
    "    df_grouped[\"y_test\"],\n",
    "    df_grouped[\"Standardized_Residual\"],\n",
    "    alpha=0.7,\n",
    "    s=30,\n",
    "    label=\"|Resíduo| / DP\",\n",
    ")\n",
    "\n",
    "# 2. Linha de Referência Crítica (Z-Score = 1.0)\n",
    "ax2.axhline(\n",
    "    y=1.0, color=\"red\", linestyle=\"-\", linewidth=2, label=\"Limite de 1 Desvio Padrão\"\n",
    ")\n",
    "ax2.axhline(y=2.0, color=\"orange\", linestyle=\"--\", linewidth=1, label=\"Limite de 2 DP\")\n",
    "\n",
    "\n",
    "ax2.set_xlabel(\"Valores Reais de r(Y_true)\", fontsize=12)\n",
    "ax2.set_ylabel(\"Resíduo Padronizado (|Y - Ŷ| / DP)\", fontsize=12)\n",
    "ax2.grid(True, linestyle=\":\", alpha=0.7)\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "# 6 - Sugestões para trabalhos futuros\n",
    "\n",
    "- Fazer uma seleção de features utilizando-se um modelo linear ao invés do método estatístico utilizado pela classe SelectKBest.\n",
    "\n",
    "- Testar o resultado do modelo eliminando menos features.\n",
    "\n",
    "- Criar novas features com os dados de entrada.\n",
    "\n",
    "- Aumentar a rede de parâmetros da validação cruzada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capacitacao-DnesvXxz-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
