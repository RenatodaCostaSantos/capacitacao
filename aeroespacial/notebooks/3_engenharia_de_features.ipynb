{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42948c34",
   "metadata": {},
   "source": [
    "# Engenharia e seleção de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7725989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib #Para salvar o modelo \n",
    "from sklearn.experimental import enable_halving_search_cv # Necessário para importar\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import Input\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import uniform, randint # Para a distribuição de parâmetros\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "import os, random\n",
    "import utils\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deefbd2",
   "metadata": {},
   "source": [
    "# Observações:\n",
    "\n",
    "1 - As bibliotecas que serão usadas neste projeto possuem fontes múltiplas de aleatoriedade. Por isso ressalta-se os seguintes pontos abaixo:\n",
    "\n",
    "- numpy, random (do Python), e tensorflow usam geradores diferentes.\n",
    "\n",
    "    - Fixar random_state em HalvingRandomSearchCV só controla o sorteio dos hiperparâmetros, não o comportamento interno da rede.\n",
    "\n",
    "- TensorFlow e paralelismo introduzem aleatóriedade.\n",
    "\n",
    "    - Por padrão, TensorFlow usa múltiplas threads e kernels otimizados (como cuDNN no GPU), que executam operações não determinísticas (principalmente Dropout e Dense).\n",
    "\n",
    "- O KerasRegressor recria o modelo a cada chamada\n",
    "\n",
    "    - Mesmo que o random_state do scikit-learn esteja fixo, cada vez que fit() é chamado, o Sequential() do TensorFlow usa um estado de aleatoriedade independente (a menos que se fixe isso manualmente dentro da função que cria o modelo).\n",
    "\n",
    "Dadas as observações acima, fixa-se abaixo o parâmetro SEED para alguns métodos que serão utilizados neste notebook.\n",
    "\n",
    "\n",
    "\n",
    "2 - Ao rodar o processo de criação do modelo, na minha máquina, com o parâmetro n_jobs diferente de 1, havia o seguinte warning:\n",
    "\n",
    "   \n",
    "    UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. \n",
    "   \n",
    "\n",
    "- Ao parar, todo o processo de seleção de parâmetros ótimos era comprometido e não havia reproducibilidade do modelo à cada rodada.\n",
    "\n",
    "- Colocando o n_jobs = 1, garante-se a reproducibilidade do modelo, ao custo de aumentar bastante o tempo de treinamento do modelo\n",
    "\n",
    "- Caso esse modelo seja rodado em uma máquina mais robusta, esse problema pode não ocorrer, sendo possível acelerar o tempo de treinamento ao mudar-se o valor do parâmetro n_jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e3c880",
   "metadata": {},
   "source": [
    "# Parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6794026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desativar GPU (garante total determinismo)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "# Fixar sementes globais\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Garante execução determinística, desativando otimizações não reprodutíveis\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "# Numero de neuronios de entrada na rede neural\n",
    "neuronios_entrada = 100\n",
    "\n",
    "n_jobs = 1\n",
    "\n",
    "caminho_features_selecionadas = '../data/08_reporting/selected_non_linear_features.npy'\n",
    "\n",
    "caminho_dados = '../data/02_intermediate/training_b_df.csv'\n",
    "\n",
    "caminho_modelo = '../data/06_models/modelo_notebook_3.joblib'\n",
    "\n",
    "caminho_shap = '../data/08_reporting/shap_values_calculados.npy'\n",
    "\n",
    "caminho_X_train_non_linear_features = '../data/03_primary/X_train_non_linear_features.npy'\n",
    "\n",
    "caminho_y_train_non_linear_features = '../data/03_primary/y_train_non_linear_features.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754bcd2f",
   "metadata": {},
   "source": [
    "# 1 - Lendo os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79504538",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(caminho_dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5c7fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.drop(columns=['r'])\n",
    "y_train = df['r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b269011",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c620fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ed5e64",
   "metadata": {},
   "source": [
    "# 2 - Engenharia de features\n",
    "\n",
    "Ao conversar com a astrofísica Camila Novaes que forneceu os dados simulados estudados aqui, foi sugerida a criação da média ponderada abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4f2d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = utils.cria_feature_media_ponderada(X_train)\n",
    "\n",
    "print(\"Primeiras 5 linhas com a nova feature:\")\n",
    "print(X_train[['cl_2', 'cl_3', 'cl_4', 'cl_511', 'media_ponderada_Cls']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c7d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['media_ponderada_Cls'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c89aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['media_ponderada_Cls'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a787d8",
   "metadata": {},
   "source": [
    "### Interpretação Cosmológica da Feature\n",
    "\n",
    "A feature media_ponderada_Cls é um indicador robusto da amplitude média da potência no regime das primeiras oscilações acústicas.\n",
    "\n",
    "    O intervalo de ℓ=2 a ℓ=511 (que vai de grandes escalas angulares até o final do primeiro pico acústico) resume a força e a altura da primeira grande oscilação de compressão/expansão no plasma primordial.\n",
    "\n",
    "    Uma media_ponderada_Cls maior implica que o espectro de potência é, em média, mais alto neste regime, o que geralmente se traduz em um universo com maior variação de temperatura (mais estrutura inicial)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8cbd4a",
   "metadata": {},
   "source": [
    "# 3 - Seleção de features\n",
    "\n",
    "Para a seleção de features utiliza-se o seguinte fluxo:\n",
    "\n",
    "1 - Utiliza-se o método estatístico de seleção de features, SelectKBest do scikit-learn para se verificar correlações entre as features e o target.\n",
    "\n",
    "2 - Mantém-se apenas as 30 features mais importantes.\n",
    "\n",
    "3 - Verifica-se o desempenho da rede neural no conjunto de teste utilizando-se apenas as features selecionadas e compara-se com o desempenho obtido utilizando-se todas as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35852578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instanciar o seletor.\n",
    "k_best = SelectKBest(score_func=f_regression, k=neuronios_entrada)\n",
    "\n",
    "# 2. Ajustar o seletor aos dados de TREINO (X e y)\n",
    "k_best.fit(X_train, y_train.to_numpy().ravel())\n",
    "\n",
    "# 3. Obter os scores e os índices das colunas selecionadas\n",
    "scores = k_best.scores_\n",
    "selected_features_indices = k_best.get_support(indices=True)\n",
    "selected_feature_names = X_train.columns[selected_features_indices]\n",
    "\n",
    "print(\"Features Selecionadas:\", selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b362d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O array 'scores' (F-statistic) tem o mesmo tamanho que X_train.columns\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance_Score': scores  # Este é o F-statistic\n",
    "})\n",
    "\n",
    "# Ordenar o DataFrame pelo Score de Importância em ordem decrescente\n",
    "importance_df = importance_df.sort_values(by='Importance_Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "K_FEATURES = 30\n",
    "top_k_features_df = importance_df.head(K_FEATURES)\n",
    "\n",
    "# Visualização (Gráfico de Barras) ---\n",
    "plt.figure(figsize=(15, 8))\n",
    "# Usamos o seaborn para um visual melhor\n",
    "sns.barplot(x='Importance_Score', y='Feature', data=top_k_features_df, palette='viridis', hue= 'Feature', legend=False)\n",
    "\n",
    "plt.title(f'Top {K_FEATURES} Features Selecionadas (Score F-regression)')\n",
    "plt.xlabel('F-statistic (Score de Importância)')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f22a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe com a importancia de todas as features selecionadas para treinar o modelo\n",
    "k_features_df = importance_df.head(neuronios_entrada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2439880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Força a visualização de todas as linhas\n",
    "pd.set_option('display.max_rows', None)\n",
    "k_features_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f4b22a",
   "metadata": {},
   "source": [
    "## Observações\n",
    "\n",
    "- As 100 features mais importantes não possuem valores de ℓ para os Cℓ​'s maior que 120. Isso corrobora com o conhecimento teórico de que a maior parte da informação cosmológica (como os picos acústicos) está concentrada nas baixas frequências (grandes escalas angulares), correspondentes aos ℓ's menores.\n",
    "\n",
    "- O conhecimento teórico aponta para o valor Cℓ=10​ (ou Cℓ​ em torno de ℓ=10, o chamado plateau de baixa ℓ) como sendo de importância fundamental, pois carrega a informação sobre a reionização. Esse comportamento não é observado acima, onde outras Cℓ​'s dominaram o ranking. Esse resultado pode ser visto de duas maneiras:\n",
    "\n",
    "    1. Os dados simulados apresentam alguma inconsistência ou viés estrutural.\n",
    "\n",
    "    2. Outros Cℓ​'s, diferentes dos previstos, fornecem mais informações sobre o valor do alvo 'r' (Razão Tensor-Escalar) do que o esperado pela teoria.\n",
    "\n",
    "- Ambos os resultados acima podem ser valiosos para os astrofísicos e cosmólogos que estejam trabalhando nesta área, pois indicam quais escalas angulares são mais sensíveis à variação do parâmetro 'r' dentro do conjunto de dados utilizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e35d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reseta a visualização para o máximo de 10 linhas\n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a54f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[selected_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64318718",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bbeaac",
   "metadata": {},
   "source": [
    "## Observação\n",
    "\n",
    "Para plotar os gráficos SHAP com a explicabilidade dos modelos, salva-se abaixo as features usadas na modelagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52afc217",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acafa6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fa026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(caminho_features_selecionadas, 'wb') as f:\n",
    "    pickle.dump({'list': selected_features}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356ab4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checa se o arquivo foi salvo corretamente\n",
    "with open(caminho_features_selecionadas, 'rb') as f:\n",
    "    loaded_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a869127",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data['list']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c221dd5",
   "metadata": {},
   "source": [
    "## 4 - Normalizando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae07d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Escalonamento das Features (X) \n",
    "X_scaler = MinMaxScaler()\n",
    "X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "\n",
    "# 2. Escalonamento do Target (Y), se for Regressão Contínua \n",
    "# Reformatar y_train para que o scaler funcione (de Series para 2D array/DataFrame)\n",
    "y_train_2d = y_train.values.reshape(-1, 1)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train_2d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb184d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367aa1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a28453",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cd3f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_scaled.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1800591",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_scaled.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0526217",
   "metadata": {},
   "source": [
    "## Salvando valores normalizados\n",
    "\n",
    "As features normalizadas serão necessárias no notebook de explicabilidade dos modelos, por isso salva-se abaixo:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff659c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(caminho_X_train_non_linear_features, X_train_scaled)\n",
    "\n",
    "np.save(caminho_y_train_non_linear_features, y_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b9a9c",
   "metadata": {},
   "source": [
    "# 4 - Construindo um modelo de redes neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fcca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma função que já “fixa” neuronios_entrada\n",
    "modelo_com_input = partial(utils.criar_modelo_regularizado, neuronios_entrada=neuronios_entrada)\n",
    "\n",
    "# 1. Distribuição de Parâmetros a serem testados (ranges ao invés de listas fixas)\n",
    "param_distributions = {\n",
    "    # Neurônios: número inteiro aleatório entre 32 e 256\n",
    "    'model__neurons': randint(low=32, high=256), \n",
    "    \n",
    "    # Taxa de Aprendizado: valor contínuo aleatório em escala logarítmica\n",
    "    # Ex: entre 1e-4 e 1e-2 (0.0001 e 0.01)\n",
    "    'model__learning_rate': uniform(loc=0.0001, scale=0.0099), \n",
    "    \n",
    "    # Batch Size: valores discretos\n",
    "    'batch_size': [16, 32, 64],\n",
    "\n",
    "    # O dropout_rate ajuda a diminuir o overfitting\n",
    "    'model__dropout_rate': uniform(loc=0.1, scale=0.4), # Testar entre 10% e 50%\n",
    "    \n",
    "    # Epochs: valores discretos (o HRS vai descartar os piores cedo)\n",
    "    'epochs': [5, 10, 20] \n",
    "}\n",
    "\n",
    "# 2. Configurar o KerasRegressor\n",
    "nn_model = KerasRegressor(model=modelo_com_input, \n",
    "                          verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf86eac",
   "metadata": {},
   "source": [
    "## Atenção\n",
    "\n",
    "- Ao juntar as features com o target, foram colocadas 10 simulações associadas ao mesmo target. \n",
    "\n",
    "- Ao usar a validação cruzada é preciso ter certeza de que as linhas associadas a um dado valor do target, caiam tanto na validação quanto no treino, evitando assim o vazamento do target. \n",
    "\n",
    "- Para isso utiliza-se o parâmetro `cv=gkf` no HalvingRandomSearch abaixo.\n",
    "\n",
    "- A variável gkf usa a classe GroupKFold para levar em consideração a observação acima e evitar o vazamento do target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aba083",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = y_train.to_numpy().ravel() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b96a5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afb8fb9",
   "metadata": {},
   "source": [
    "Abaixo, verifica-se se realmente não há vazamento de target usando-se o GrupoKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialize o GroupKFold\n",
    "gkf = GroupKFold(n_splits=5) \n",
    "\n",
    "# Itere sobre os splits (apenas a primeira dobra é suficiente para checar)\n",
    "for fold, (train_index, val_index) in enumerate(gkf.split(X_train, y_train, groups=groups)):\n",
    "    \n",
    "    if fold == 0: # Checar apenas o primeiro fold\n",
    "        \n",
    "        # 1. Obter os valores de 'r' (originais, não escalados) para Treino e Validação\n",
    "        r_train_fold = y_train[train_index].to_numpy().ravel()\n",
    "        r_val_fold = y_train[val_index].to_numpy().ravel()\n",
    "        \n",
    "        # 2. Encontrar os valores ÚNICOS de 'r' em cada conjunto\n",
    "        r_unique_train = set(r_train_fold)\n",
    "        r_unique_val = set(r_val_fold)\n",
    "        \n",
    "        # 3. Encontrar a Interseção (os valores vazados)\n",
    "        vazamentos = r_unique_train.intersection(r_unique_val)\n",
    "        \n",
    "        print(f\"--- Checagem do Fold {fold + 1} ---\")\n",
    "        print(f\"Total de valores únicos de 'r' no Treino: {len(r_unique_train)}\")\n",
    "        print(f\"Total de valores únicos de 'r' na Validação: {len(r_unique_val)}\")\n",
    "        print(f\"Valores de 'r' vazando (Interseção): {len(vazamentos)}\")\n",
    "        \n",
    "        if len(vazamentos) == 0:\n",
    "            print(\"✅ GroupKFold está funcionando corretamente: Nenhuma intersecção de valores de 'r'.\")\n",
    "        else:\n",
    "            print(f\"❌ ERRO GRAVE: {len(vazamentos)} valores de 'r' estão vazando! O GroupKFold falhou na divisão dos grupos.\")\n",
    "            print(f\"Valores vazados (Primeiros 5): {list(vazamentos)[:5]}\")\n",
    "            \n",
    "        break # Parar após o primeiro fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e035a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# factor=2: Descarta metade dos candidatos a cada iteração.\n",
    "# candidates: O número total de combinações que serão testadas na 1ª rodada (a maior).\n",
    "hrs = HalvingRandomSearchCV(\n",
    "    estimator=nn_model, \n",
    "    param_distributions=param_distributions, \n",
    "    factor=2, \n",
    "    n_candidates=50,\n",
    "    scoring='r2', \n",
    "    random_state=42,\n",
    "    cv=gkf, \n",
    "    verbose=2,\n",
    "    n_jobs=n_jobs\n",
    ")\n",
    "\n",
    "print(\"Iniciando Halving Random Search (Testando as melhores combinações eficientemente)...\")\n",
    "\n",
    "# 4. Executar a busca\n",
    "# O HRS executa a busca e o retreinamento (refit=True)\n",
    "hrs_result = hrs.fit(X_train_scaled, y_train_scaled, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d0f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESULTADOS FINAIS DO HALVING RANDOM SEARCH\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f'Número de features usadas: {len(selected_feature_names)}')\n",
    "\n",
    "# Melhor pontuação\n",
    "print(f\"Melhor R² Médio: {hrs_result.best_score_:.4f}\")\n",
    "\n",
    "# Melhor combinação de hiperparâmetros\n",
    "print(\"Melhores Parâmetros:\")\n",
    "print(hrs_result.best_params_)\n",
    "\n",
    "# Obter o melhor modelo treinado\n",
    "best_nn_model = hrs_result.best_estimator_\n",
    "\n",
    "# Salva o  modelo gerado\n",
    "joblib.dump(best_nn_model, caminho_modelo)\n",
    "\n",
    "# --- AVALIAÇÃO FINAL ---\n",
    "\n",
    "\n",
    "# O reshape é necessário pois o KerasRegressor.predict() retorna 1D por padrão no Scikit-Learn Wrapper\n",
    "y_pred_scaled = best_nn_model.predict(X_train_scaled).reshape(-1, 1)\n",
    "\n",
    "# Faz a transformação inversa para obter os valores em 'r':\n",
    "y_pred_original = y_scaler.inverse_transform(y_pred_scaled)\n",
    "\n",
    "# 4. Prepara o target verdadeiro para comparação. Garante que y_train seja convertido de um pandas Series para um Numpy array, que é o objeto que sai da inverse_transform acima\n",
    "y_true = y_train.values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Cálculo das métricas no conjunto de treino\n",
    "final_mse = mean_squared_error(y_true, y_pred_original) \n",
    "final_rmse = root_mean_squared_error(y_true, y_pred_original) \n",
    "final_r2 = r2_score(y_true, y_pred_original)\n",
    "final_mae = mean_absolute_error(y_true, y_pred_original) \n",
    "\n",
    "\n",
    "print(\"\\nMétricas do Melhor Modelo (Avaliadas nos dados completos de Treino):\")\n",
    "print(f\"  MSE (Erro Quadrático Médio): {final_mse:.8f} (Penaliza erros grandes)\")\n",
    "print(f\"  RMSE (Erro Quadrático Médio): {final_rmse:.8f} (Penaliza erros grandes)\")\n",
    "print(f\"  MAE (Erro Absoluto Médio):   {final_mae:.8f} (Unidade de 'r')\")\n",
    "print(f\"  R2 (Ajuste):                 {final_r2:.4f} (Qualidade do ajuste)\")\n",
    "print(\"=\"*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c67575",
   "metadata": {},
   "source": [
    "## Observação\n",
    "\n",
    "- Comparado com os resultados nos dados de treino do notebook 2, todas as métricas melhoraram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c643255",
   "metadata": {},
   "source": [
    "## 4.1 - Visualizando os resultados\n",
    "\n",
    "Abaixo, são mostradas duas visualizações:\n",
    "\n",
    "- A distribuição dos valores preditos comparados com os reais,\n",
    "\n",
    "- A distribuição dos resíduos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad166a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2987c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf29544",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_original.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dbcb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6527b49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Suponha que você já tem:\n",
    "# y_true_original: valores verdadeiros de 'r' (na escala original)\n",
    "# y_pred_original: valores previstos de 'r' (na escala original)\n",
    "\n",
    "# Crie a linha de identidade X=Y\n",
    "min_val = min(y_train.min(), y_pred_original.min())\n",
    "max_val = max(y_train.max(), y_pred_original.max())\n",
    "ideal_line = np.linspace(min_val, max_val, 100)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# 1. Scatter Plot dos Resultados\n",
    "plt.scatter(y_train, y_pred_original, alpha=0.6, s=20, label='Previsões')\n",
    "\n",
    "# 2. Linha de Identidade (Ajuste Perfeito)\n",
    "plt.plot(ideal_line, ideal_line, color='red', linestyle='--', linewidth=2, label='Ajuste Perfeito (Y=X)')\n",
    "\n",
    "plt.title(f'Previsões vs. Valores Reais de r (R² Final: {final_r2:.4f})', fontsize=14)\n",
    "plt.xlabel('Valores Reais de r', fontsize=12)\n",
    "plt.ylabel('Valores Previstos de r', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=':', alpha=0.7)\n",
    "plt.gca().set_aspect('equal', adjustable='box') # Garante que os eixos tenham a mesma escala\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d5b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcule os resíduos (True - Predicted)\n",
    "residuals = y_true - y_pred_original\n",
    "\n",
    "# Achata o array para 1D (o formato (N,))\n",
    "residuals_1d = residuals.ravel()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Histograma dos resíduos\n",
    "plt.hist(residuals_1d, bins=30, edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Linha vertical em zero (onde o centro do histograma deveria estar)\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Erro Zero')\n",
    "\n",
    "plt.title('Distribuição dos Resíduos (Erros)', fontsize=14)\n",
    "plt.xlabel('Resíduo (Real r - Previsto r)', fontsize=12)\n",
    "plt.ylabel('Frequência', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=':', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8744341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PREPARAÇÃO ---\n",
    "# Criando um DataFrame para facilitar a agregação\n",
    "# Garantindo que y_train e y_pred_original são 1D para o DataFrame\n",
    "y_true_flat = y_true.ravel()\n",
    "y_pred_flat = y_pred_original.ravel()\n",
    "\n",
    "df_results = pd.DataFrame({\n",
    "    'y_true': y_true_flat,\n",
    "    'y_pred': y_pred_flat\n",
    "})\n",
    "\n",
    "# AGREGAR: Agrupar por 'y_true' e calcular Média e Desvio Padrão para 'y_pred'\n",
    "# 'y_true' (a variável agrupada) será o índice do novo DataFrame\n",
    "df_grouped = df_results.groupby('y_true')['y_pred'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# Renomear as colunas para clareza\n",
    "df_grouped.columns = ['y_true', 'y_pred_mean', 'y_pred_std']\n",
    "\n",
    "\n",
    "# Cálculo do Resíduo Padronizado (em módulos, conforme solicitado)\n",
    "# Evite a divisão por zero, substituindo desvios padrão zero por um valor pequeno (ex: 1e-6)\n",
    "df_grouped['y_pred_std_safe'] = df_grouped['y_pred_std'].replace(0, 1e-6)\n",
    "df_grouped['Standardized_Residual'] = (\n",
    "    np.abs(df_grouped['y_true'] - df_grouped['y_pred_mean']) / df_grouped['y_pred_std_safe']\n",
    ")\n",
    "\n",
    "# Exibir o resultado da agregação (opcional)\n",
    "print(\"Dados Agrupados (Exemplo):\")\n",
    "print(df_grouped.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1797cc",
   "metadata": {},
   "source": [
    "## Visualização com barra de erros\n",
    "\n",
    "Os dados utilizados associam 10 valores diferentes das features a um mesmo valor do target. O objetivo é simular os ruídos presentes em observações astronômicas.\n",
    "\n",
    "Uma forma mais útil de visualizar os dados é portanto colapsar os 10 dados referentes à cada valor do target em seu valor médio e desvio padrão.\n",
    "\n",
    "Além disso, plota-se gráfico de resíduos padronizados, que ajuda  visualizar o número de observações que se distanciam do valor real em mais de um desvio-padrão.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305d4aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um espaço de figuras com 2 linhas e 1 coluna, compartilhando o eixo X\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    2, 1, \n",
    "    figsize=(8, 10), \n",
    "    sharex=True, # Os dois gráficos compartilham o mesmo eixo X\n",
    "    gridspec_kw={'hspace': 0.05}, # Reduz o espaço entre os subplots\n",
    "    constrained_layout=True \n",
    ")\n",
    "\n",
    "# ==========================================================\n",
    "# GRÁFICO SUPERIOR: Previsão Média vs. Real (Com Barras de Erro)\n",
    "# ==========================================================\n",
    "\n",
    "# 1. Plotar os pontos agregados com Barras de Erro\n",
    "ax1.errorbar(\n",
    "    x=df_grouped['y_true'],             \n",
    "    y=df_grouped['y_pred_mean'],            \n",
    "    yerr=df_grouped['y_pred_std'],          \n",
    "    fmt='o',                                \n",
    "    capsize=4,                              \n",
    "    alpha=0.7,\n",
    "    label='Previsão Média ± 1 DP'\n",
    ")\n",
    "\n",
    "# 2. Linha de Identidade (Ajuste Perfeito Y=X)\n",
    "ax1.plot(ideal_line, ideal_line, color='red', linestyle='--', linewidth=2, label='Ajuste Perfeito (Y=X)')\n",
    "\n",
    "ax1.set_title(f'Análise de Previsão Agregada (R² Final: {final_r2:.4f})', fontsize=14)\n",
    "ax1.set_ylabel('Valores Previstos Médios de r', fontsize=12)\n",
    "ax1.grid(True, linestyle=':', alpha=0.7)\n",
    "ax1.legend()\n",
    "\n",
    "# ==========================================================\n",
    "# GRÁFICO INFERIOR: Resíduos Padronizados\n",
    "# ==========================================================\n",
    "\n",
    "# 1. Scatter Plot dos Resíduos Padronizados\n",
    "ax2.scatter(\n",
    "    df_grouped['y_true'], \n",
    "    df_grouped['Standardized_Residual'], \n",
    "    alpha=0.7, \n",
    "    s=30, \n",
    "    label='|Resíduo| / DP'\n",
    ")\n",
    "\n",
    "# 2. Linha de Referência Crítica (Z-Score = 1.0)\n",
    "ax2.axhline(\n",
    "    y=1.0, \n",
    "    color='red', \n",
    "    linestyle='-', \n",
    "    linewidth=2, \n",
    "    label='Limite de 1 Desvio Padrão'\n",
    ")\n",
    "ax2.axhline(y=2.0, color='orange', linestyle='--', linewidth=1, label='Limite de 2 DP')\n",
    "\n",
    "\n",
    "ax2.set_xlabel('Valores Reais de r (Y_true)', fontsize=12)\n",
    "ax2.set_ylabel('Resíduo Padronizado (|Y - Ŷ| / DP)', fontsize=12)\n",
    "ax2.grid(True, linestyle=':', alpha=0.7)\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb43a7aa",
   "metadata": {},
   "source": [
    "# Versão do gráfico em inglês"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf001b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure space with 2 rows and 1 column, sharing the X-axis\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    2, 1, \n",
    "    figsize=(8, 10), \n",
    "    sharex=True, # The two plots share the same X-axis\n",
    "    gridspec_kw={'hspace': 0.05}, # Reduce space between subplots\n",
    "    constrained_layout=True \n",
    ")\n",
    "\n",
    "# ==========================================================\n",
    "# UPPER PLOT: Mean Prediction vs. True (With Error Bars)\n",
    "# ==========================================================\n",
    "\n",
    "# 1. Plot the aggregated points with Error Bars\n",
    "ax1.errorbar(\n",
    "    x=df_grouped['y_true'],             \n",
    "    y=df_grouped['y_pred_mean'],            \n",
    "    yerr=df_grouped['y_pred_std'],          \n",
    "    fmt='o',                                \n",
    "    capsize=4,                              \n",
    "    alpha=0.7,\n",
    "    label='Mean Prediction ± 1 SD'\n",
    ")\n",
    "\n",
    "# 2. Line of Identity (Perfect Fit Y=X)\n",
    "ax1.plot(ideal_line, ideal_line, color='red', linestyle='--', linewidth=2, label='Perfect Fit (Y=X)')\n",
    "\n",
    "ax1.set_title(f'Aggregate Prediction Analysis (Final R²: {final_r2:.4f})', fontsize=14)\n",
    "ax1.set_ylabel('Mean Predicted r Values', fontsize=12)\n",
    "ax1.grid(True, linestyle=':', alpha=0.7)\n",
    "ax1.legend()\n",
    "\n",
    "# ==========================================================\n",
    "# LOWER PLOT: Standardized Residuals\n",
    "# ==========================================================\n",
    "\n",
    "# 1. Scatter Plot of Standardized Residuals\n",
    "ax2.scatter(\n",
    "    df_grouped['y_true'], \n",
    "    df_grouped['Standardized_Residual'], \n",
    "    alpha=0.7, \n",
    "    s=30, \n",
    "    label='|Residual| / SD'\n",
    ")\n",
    "\n",
    "# 2. Critical Reference Line (Z-Score = 1.0)\n",
    "ax2.axhline(\n",
    "    y=1.0, \n",
    "    color='red', \n",
    "    linestyle='-', \n",
    "    linewidth=2, \n",
    "    label='1 Standard Deviation Limit'\n",
    ")\n",
    "ax2.axhline(y=2.0, color='orange', linestyle='--', linewidth=1, label='2 SD Limit')\n",
    "\n",
    "\n",
    "ax2.set_xlabel('True r Values (Y_true)', fontsize=12)\n",
    "ax2.set_ylabel('Standardized Residual (|Y - Ŷ| / SD)', fontsize=12)\n",
    "ax2.grid(True, linestyle=':', alpha=0.7)\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capacitacao-DnesvXxz-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
